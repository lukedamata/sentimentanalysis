{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Sentiment data shape: (11929999, 21)\n",
      "Return data shape: (2459589, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "# Replace these paths with the actual data file paths\n",
    "sentiment_data_path = \"sentiment_train_2017_2021.csv\"\n",
    "return_data_path = \"return_train_2017_2021.csv\"\n",
    "\n",
    "# Load the data\n",
    "sentiment_data = pd.read_csv(sentiment_data_path)\n",
    "return_data = pd.read_csv(return_data_path)\n",
    "\n",
    "# Display basic shapes\n",
    "print(\"Sentiment data shape:\", sentiment_data.shape)\n",
    "print(\"Return data shape:\", return_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  StoryID                Post_Time        Received_Time  \\\n",
      "0  RR20170101VFVFDB4TGnM=  2017-01-01 00:03:09.000  2017-01-01 00:05:09   \n",
      "1  RR20170101VFVFDBsVG3M=  2017-01-01 00:06:59.000  2017-01-01 00:08:59   \n",
      "2  RR20170101VFVFD1RXCXM=  2017-01-01 00:15:36.000  2017-01-01 00:17:36   \n",
      "3  RR20170101VFVFD1QMCnM=  2017-01-01 00:15:56.000  2017-01-01 00:17:56   \n",
      "4  RR20170101VFVFDg9QCHM=  2017-01-01 00:45:04.000  2017-01-01 00:47:04   \n",
      "\n",
      "  Ticker Country          ISIN  Relevance  Sentiment  Confidence  Prob_POS  \\\n",
      "0   HOOD     USA  US7707001027   1.000000          0    0.874448  0.041390   \n",
      "1    PRI     USA  US74164M1080   1.000000         -1    0.709253  0.059022   \n",
      "2    BHE     USA  US08160H1014   1.000000          0    0.461974  0.021243   \n",
      "3   LOGM     USA  US54142L1098   0.703235         -1    0.531746  0.068879   \n",
      "4    SUN     USA  US86765K1097   0.254271          1    0.212626  0.475079   \n",
      "\n",
      "   ...  Reddit_Topic  TopicWeight      Alex_Topic          Source  \\\n",
      "0  ...           NaN    -0.017089         AA@ALEX  investing_RSRR   \n",
      "1  ...           NaN    -0.017089         AA@ALEX        wsb_RSRR   \n",
      "2  ...          Help    -0.029671  AA@ALEX,AA@ERN  investing_RSRR   \n",
      "3  ...           NaN    -0.017089         AA@ALEX         wsb_RSR   \n",
      "4  ...           NaN    -0.017089  AA@OPS,AA@ALEX        wsb_RSRR   \n",
      "\n",
      "  SourceWeight  LinkID                            Author Novelty  \\\n",
      "0     0.039016  5l882m  e9450a943f651434728bf39a643d9036       1   \n",
      "1    -0.000515  5l6kro  d0226c99ff38dd5e6cb87d7766dc798f       1   \n",
      "2     0.039016  5lbz7h  f33c6e9d637b862f4289aaaf6555b498       1   \n",
      "3     0.007037  5lcb5w  6de93d1b8410255dcdc19d5067db1eac       1   \n",
      "4    -0.000515  5lc6t6  6de93d1b8410255dcdc19d5067db1eac       1   \n",
      "\n",
      "  Comment_Count       Date  \n",
      "0             1 2017-01-01  \n",
      "1             1 2017-01-01  \n",
      "2             2 2017-01-01  \n",
      "3             1 2017-01-01  \n",
      "4             1 2017-01-01  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "def ensure_date_column(df):\n",
    "    if 'Date' not in df.columns:\n",
    "        if 'Received_Time' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['Received_Time']).dt.floor('D')\n",
    "        elif 'Post_Time' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['Post_Time']).dt.floor('D')\n",
    "        else:\n",
    "            raise KeyError(\"No 'Date', 'Received_Time', or 'Post_Time' column found in sentiment data.\")\n",
    "    else:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    return df\n",
    "\n",
    "sentiment_data = ensure_date_column(sentiment_data)\n",
    "return_data['Date'] = pd.to_datetime(return_data['Date'])\n",
    "\n",
    "# Optional: display a sample to verify\n",
    "print(sentiment_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(sentiment_data, return_data=None, training=True):\n",
    "    \"\"\"\n",
    "    Engineer features from sentiment data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sentiment_data : DataFrame\n",
    "        The Reddit sentiment data\n",
    "    return_data : DataFrame, optional\n",
    "        The stock return data (used during training)\n",
    "    training : bool, default True\n",
    "        Whether this is for training or prediction\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    features_df : DataFrame\n",
    "        DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    # Ensure the 'Date' column exists\n",
    "    sentiment_data = ensure_date_column(sentiment_data)\n",
    "    \n",
    "    # Copy the data to avoid modifying the original\n",
    "    sentiment = sentiment_data.copy()\n",
    "    \n",
    "    # Group by Date and Ticker\n",
    "    grouped = sentiment.groupby(['Date', 'Ticker'])\n",
    "    daily_features = []\n",
    "    \n",
    "    for (date, ticker), group in grouped:\n",
    "        if len(group) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Calculate basic sentiment statistics\n",
    "        mean_sentiment = group['Sentiment'].mean()\n",
    "        median_sentiment = group['Sentiment'].median()\n",
    "        std_sentiment = group['Sentiment'].std() or 0\n",
    "        min_sentiment = group['Sentiment'].min()\n",
    "        max_sentiment = group['Sentiment'].max()\n",
    "        post_count = len(group)\n",
    "        log_post_count = np.log1p(post_count)\n",
    "        mean_prob_pos = group['Prob_POS'].mean()\n",
    "        mean_prob_neg = group['Prob_NEG'].mean()\n",
    "        mean_prob_ntr = group['Prob_NTR'].mean()\n",
    "        max_probs = group[['Prob_POS', 'Prob_NEG', 'Prob_NTR']].max(axis=1)\n",
    "        mean_certainty = max_probs.mean()\n",
    "        \n",
    "        def entropy(row):\n",
    "            probs = [row['Prob_POS'], row['Prob_NEG'], row['Prob_NTR']]\n",
    "            probs = [p for p in probs if p > 0]\n",
    "            return -sum(p * np.log(p) for p in probs)\n",
    "        mean_entropy = group.apply(entropy, axis=1).mean()\n",
    "        \n",
    "        unique_authors = group['Author'].nunique()\n",
    "        author_ratio = unique_authors / post_count\n",
    "        pos_ratio = (group['Sentiment'] > 0).mean()\n",
    "        neg_ratio = (group['Sentiment'] < 0).mean()\n",
    "        ntr_ratio = (group['Sentiment'] == 0).mean()\n",
    "        sentiment_skew = group['Sentiment'].skew() or 0\n",
    "        weighted_sentiment = (group['Sentiment'] * group['Prob_POS']).sum() / group['Prob_POS'].sum() \\\n",
    "            if group['Prob_POS'].sum() > 0 else mean_sentiment\n",
    "        \n",
    "        features = {\n",
    "            'Date': date,\n",
    "            'Ticker': ticker,\n",
    "            'mean_sentiment': mean_sentiment,\n",
    "            'median_sentiment': median_sentiment,\n",
    "            'std_sentiment': std_sentiment,\n",
    "            'min_sentiment': min_sentiment,\n",
    "            'max_sentiment': max_sentiment,\n",
    "            'sentiment_range': max_sentiment - min_sentiment,\n",
    "            'post_count': post_count,\n",
    "            'log_post_count': log_post_count,\n",
    "            'mean_prob_pos': mean_prob_pos,\n",
    "            'mean_prob_neg': mean_prob_neg,\n",
    "            'mean_prob_ntr': mean_prob_ntr,\n",
    "            'mean_certainty': mean_certainty,\n",
    "            'mean_entropy': mean_entropy,\n",
    "            'unique_authors': unique_authors,\n",
    "            'author_ratio': author_ratio,\n",
    "            'pos_ratio': pos_ratio,\n",
    "            'neg_ratio': neg_ratio,\n",
    "            'ntr_ratio': ntr_ratio,\n",
    "            'sentiment_skew': sentiment_skew,\n",
    "            'weighted_sentiment': weighted_sentiment\n",
    "        }\n",
    "        daily_features.append(features)\n",
    "    \n",
    "    features_df = pd.DataFrame(daily_features)\n",
    "    if features_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Merge with returns if training\n",
    "    if training and return_data is not None:\n",
    "        return_data['Date'] = pd.to_datetime(return_data['Date'])\n",
    "        features_df = pd.merge(\n",
    "            features_df,\n",
    "            return_data[['Date', 'Ticker', 'Return']],\n",
    "            on=['Date', 'Ticker'],\n",
    "            how='left'\n",
    "        )\n",
    "        missing_returns = features_df['Return'].isnull().sum()\n",
    "        if missing_returns > 0:\n",
    "            print(f\"Warning: {missing_returns} rows have missing returns and will be dropped.\")\n",
    "            features_df = features_df.dropna(subset=['Return'])\n",
    "    \n",
    "    return features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(sentiment_data, return_data):\n",
    "    \"\"\"\n",
    "    Train a model using sentiment features to predict next-day returns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sentiment_data : DataFrame\n",
    "        The Reddit sentiment data for training (e.g., sentiment_train_2017_2021.csv)\n",
    "    return_data : DataFrame\n",
    "        The stock return data for training (e.g., return_train_2017_2021.csv)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model_info : dict\n",
    "        Dictionary containing the trained model object and necessary metadata.\n",
    "    \"\"\"\n",
    "    # Ensure that sentiment_data has a 'Date' column.\n",
    "    def ensure_date_column(df):\n",
    "        if 'Date' not in df.columns:\n",
    "            if 'Received_Time' in df.columns:\n",
    "                df['Date'] = pd.to_datetime(df['Received_Time']).dt.floor('D')\n",
    "            elif 'Post_Time' in df.columns:\n",
    "                df['Date'] = pd.to_datetime(df['Post_Time']).dt.floor('D')\n",
    "            else:\n",
    "                raise KeyError(\"No 'Date', 'Received_Time', or 'Post_Time' column found in sentiment data.\")\n",
    "        else:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "        return df\n",
    "\n",
    "    sentiment_data = ensure_date_column(sentiment_data)\n",
    "    return_data['Date'] = pd.to_datetime(return_data['Date'])\n",
    "    \n",
    "    # Use the previously defined engineer_features function\n",
    "    features_df = engineer_features(sentiment_data, return_data, training=True)\n",
    "    \n",
    "    if features_df.empty:\n",
    "        raise ValueError(\"No features could be engineered from the provided data.\")\n",
    "    \n",
    "    # Prepare training data\n",
    "    X = features_df.drop(['Date', 'Ticker', 'Return'], axis=1)\n",
    "    y = features_df['Return']\n",
    "    \n",
    "    # Fill any remaining missing values with feature means\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    print(\"Training XGBoost model...\")\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_scaled, y)\n",
    "    \n",
    "    # Package the model along with scaler and feature names\n",
    "    model_info = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': X.columns.tolist(),\n",
    "        'model_type': 'XGBoost'\n",
    "    }\n",
    "    \n",
    "    return model_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features for training data...\n",
      "Warning: 175544 rows have missing returns and will be dropped.\n",
      "Features dataframe shape: (184284, 23)\n",
      "         Date Ticker  mean_sentiment  median_sentiment  std_sentiment  \\\n",
      "51 2017-01-03   AAPL       -0.222222              -1.0       0.942809   \n",
      "52 2017-01-03    AIG       -1.000000              -1.0       0.000000   \n",
      "53 2017-01-03    AMD        0.369565               1.0       0.798852   \n",
      "54 2017-01-03   AMGN        0.000000               0.0       1.000000   \n",
      "55 2017-01-03   AMZN       -0.363636              -1.0       0.924416   \n",
      "\n",
      "    min_sentiment  max_sentiment  sentiment_range  post_count  log_post_count  \\\n",
      "51             -1              1                2          18        2.944439   \n",
      "52             -1             -1                0           4        1.609438   \n",
      "53             -1              1                2          46        3.850148   \n",
      "54             -1              1                2           3        1.386294   \n",
      "55             -1              1                2          11        2.484907   \n",
      "\n",
      "    ...  mean_certainty  mean_entropy  unique_authors  author_ratio  \\\n",
      "51  ...        0.778159      0.574225              16      0.888889   \n",
      "52  ...        0.684628      0.712259               3      0.750000   \n",
      "53  ...        0.769956      0.570177              32      0.695652   \n",
      "54  ...        0.823076      0.365331               2      0.666667   \n",
      "55  ...        0.753420      0.594392              11      1.000000   \n",
      "\n",
      "    pos_ratio  neg_ratio  ntr_ratio  sentiment_skew  weighted_sentiment  \\\n",
      "51   0.333333   0.555556   0.111111        0.495235            0.710820   \n",
      "52   0.000000   1.000000   0.000000        0.000000           -1.000000   \n",
      "53   0.565217   0.195652   0.239130       -0.782039            0.829874   \n",
      "54   0.333333   0.333333   0.333333        0.000000            0.785347   \n",
      "55   0.272727   0.636364   0.090909        0.905306            0.577800   \n",
      "\n",
      "    Return  \n",
      "51  -0.11%  \n",
      "52   1.31%  \n",
      "53   0.00%  \n",
      "54   1.42%  \n",
      "55   0.47%  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Engineering features for training data...\")\n",
    "features_df = engineer_features(sentiment_data, return_data, training=True)\n",
    "print(\"Features dataframe shape:\", features_df.shape)\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature correlations with Return:\n",
      "Return                1.000000\n",
      "mean_prob_pos         0.016441\n",
      "mean_sentiment        0.016027\n",
      "pos_ratio             0.014095\n",
      "median_sentiment      0.013822\n",
      "mean_certainty        0.012993\n",
      "weighted_sentiment    0.012409\n",
      "unique_authors        0.011744\n",
      "post_count            0.010453\n",
      "max_sentiment         0.005932\n",
      "log_post_count        0.005897\n",
      "min_sentiment         0.001979\n",
      "sentiment_range       0.000729\n",
      "author_ratio         -0.001694\n",
      "ntr_ratio            -0.003492\n",
      "std_sentiment        -0.003508\n",
      "mean_prob_ntr        -0.005703\n",
      "sentiment_skew       -0.010114\n",
      "mean_entropy         -0.013041\n",
      "neg_ratio            -0.013582\n",
      "mean_prob_neg        -0.016124\n",
      "Name: Return, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# For example, if 'Return' is a percentage string, convert it:\n",
    "if features_df['Return'].dtype == object:\n",
    "    features_df['Return'] = features_df['Return'].str.rstrip('%').astype('float') / 100\n",
    "\n",
    "numeric_features_df = features_df.drop(['Date', 'Ticker'], axis=1)\n",
    "corr_with_return = numeric_features_df.corr()['Return'].sort_values(ascending=False)\n",
    "print(\"Feature correlations with Return:\")\n",
    "print(corr_with_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 175544 rows have missing returns and will be dropped.\n",
      "Training XGBoost model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '-0.11%'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Call the train_model function using the sentiment and return data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_info \u001b[38;5;241m=\u001b[39m train_model(sentiment_data, return_data)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Information:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 59\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(sentiment_data, return_data)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining XGBoost model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\n\u001b[1;32m     52\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     53\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_scaled, y)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Package the model along with scaler and feature names\u001b[39;00m\n\u001b[1;32m     62\u001b[0m model_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model,\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m: scaler,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_names\u001b[39m\u001b[38;5;124m'\u001b[39m: X\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     67\u001b[0m }\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/sklearn.py:1222\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1217\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[1;32m   1218\u001b[0m     xgb_model, params, feature_weights\n\u001b[1;32m   1219\u001b[0m )\n\u001b[1;32m   1221\u001b[0m evals_result: TrainingCallback\u001b[38;5;241m.\u001b[39mEvalsLog \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1222\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1223\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1224\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   1225\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m   1226\u001b[0m     group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1227\u001b[0m     qid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1228\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1229\u001b[0m     base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[1;32m   1230\u001b[0m     feature_weights\u001b[38;5;241m=\u001b[39mfeature_weights,\n\u001b[1;32m   1231\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39meval_set,\n\u001b[1;32m   1232\u001b[0m     sample_weight_eval_set\u001b[38;5;241m=\u001b[39msample_weight_eval_set,\n\u001b[1;32m   1233\u001b[0m     base_margin_eval_set\u001b[38;5;241m=\u001b[39mbase_margin_eval_set,\n\u001b[1;32m   1234\u001b[0m     eval_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1235\u001b[0m     eval_qid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m     create_dmatrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dmatrix,\n\u001b[1;32m   1237\u001b[0m     enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_categorical,\n\u001b[1;32m   1238\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1239\u001b[0m )\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1242\u001b[0m     obj: Optional[Objective] \u001b[38;5;241m=\u001b[39m _objective_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/sklearn.py:628\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    609\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    625\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    626\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m create_dmatrix(\n\u001b[1;32m    629\u001b[0m         data\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    630\u001b[0m         label\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    631\u001b[0m         group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[1;32m    632\u001b[0m         qid\u001b[38;5;241m=\u001b[39mqid,\n\u001b[1;32m    633\u001b[0m         weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    634\u001b[0m         base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[1;32m    635\u001b[0m         feature_weights\u001b[38;5;241m=\u001b[39mfeature_weights,\n\u001b[1;32m    636\u001b[0m         missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[1;32m    637\u001b[0m         enable_categorical\u001b[38;5;241m=\u001b[39menable_categorical,\n\u001b[1;32m    638\u001b[0m         feature_types\u001b[38;5;241m=\u001b[39mfeature_types,\n\u001b[1;32m    639\u001b[0m         ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    640\u001b[0m     )\n\u001b[1;32m    642\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/sklearn.py:1137\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m QuantileDMatrix(\n\u001b[1;32m   1138\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, ref\u001b[38;5;241m=\u001b[39mref, nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin\n\u001b[1;32m   1139\u001b[0m         )\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:1614\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[0m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1595\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1607\u001b[0m         )\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1610\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1611\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1612\u001b[0m         )\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(\n\u001b[1;32m   1615\u001b[0m     data,\n\u001b[1;32m   1616\u001b[0m     ref\u001b[38;5;241m=\u001b[39mref,\n\u001b[1;32m   1617\u001b[0m     label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[1;32m   1618\u001b[0m     weight\u001b[38;5;241m=\u001b[39mweight,\n\u001b[1;32m   1619\u001b[0m     base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[1;32m   1620\u001b[0m     group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[1;32m   1621\u001b[0m     qid\u001b[38;5;241m=\u001b[39mqid,\n\u001b[1;32m   1622\u001b[0m     label_lower_bound\u001b[38;5;241m=\u001b[39mlabel_lower_bound,\n\u001b[1;32m   1623\u001b[0m     label_upper_bound\u001b[38;5;241m=\u001b[39mlabel_upper_bound,\n\u001b[1;32m   1624\u001b[0m     feature_weights\u001b[38;5;241m=\u001b[39mfeature_weights,\n\u001b[1;32m   1625\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[1;32m   1626\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39mfeature_types,\n\u001b[1;32m   1627\u001b[0m     enable_categorical\u001b[38;5;241m=\u001b[39menable_categorical,\n\u001b[1;32m   1628\u001b[0m     max_quantile_blocks\u001b[38;5;241m=\u001b[39mmax_quantile_batches,\n\u001b[1;32m   1629\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:1678\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[0;34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[0m\n\u001b[1;32m   1663\u001b[0m config \u001b[38;5;241m=\u001b[39m make_jcargs(\n\u001b[1;32m   1664\u001b[0m     nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnthread,\n\u001b[1;32m   1665\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1666\u001b[0m     max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin,\n\u001b[1;32m   1667\u001b[0m     max_quantile_blocks\u001b[38;5;241m=\u001b[39mmax_quantile_blocks,\n\u001b[1;32m   1668\u001b[0m )\n\u001b[1;32m   1669\u001b[0m ret \u001b[38;5;241m=\u001b[39m _LIB\u001b[38;5;241m.\u001b[39mXGQuantileDMatrixCreateFromCallback(\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1671\u001b[0m     it\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1676\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mbyref(handle),\n\u001b[1;32m   1677\u001b[0m )\n\u001b[0;32m-> 1678\u001b[0m it\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[1;32m   1680\u001b[0m _check_call(ret)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:572\u001b[0m, in \u001b[0;36mDataIter.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    570\u001b[0m exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:553\u001b[0m, in \u001b[0;36mDataIter._handle_exception\u001b[0;34m(self, fn, dft_ret)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn()\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:640\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext(input_data)), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/data.py:1654\u001b[0m, in \u001b[0;36mSingleBatchInternalIter.next\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m   1652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1654\u001b[0m input_data(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:629\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.input_data\u001b[0;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m (new, cat_codes, feature_names, feature_types)\n\u001b[1;32m    628\u001b[0m dispatch_proxy_set_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy, new, cat_codes)\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mset_info(\n\u001b[1;32m    630\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[1;32m    631\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39mfeature_types,\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    633\u001b[0m )\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_ref \u001b[38;5;241m=\u001b[39m ref\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:961\u001b[0m, in \u001b[0;36mDMatrix.set_info\u001b[0;34m(self, label, weight, base_margin, group, qid, label_lower_bound, label_upper_bound, feature_names, feature_types, feature_weights)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_meta_backend\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_label(label)\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_weight(weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:1099\u001b[0m, in \u001b[0;36mDMatrix.set_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set label of dmatrix\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \n\u001b[1;32m   1092\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;124;03m    The label information to be set into DMatrix\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_meta_backend\n\u001b[0;32m-> 1099\u001b[0m dispatch_meta_backend(\u001b[38;5;28mself\u001b[39m, label, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/data.py:1603\u001b[0m, in \u001b[0;36mdispatch_meta_backend\u001b[0;34m(matrix, data, name, dtype)\u001b[0m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_series(data):\n\u001b[0;32m-> 1603\u001b[0m     _meta_from_pandas_series(data, name, dtype, handle)\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_dlpack(data):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/xgboost/data.py:708\u001b[0m, in \u001b[0;36m_meta_from_pandas_series\u001b[0;34m(data, name, dtype, handle)\u001b[0m\n\u001b[1;32m    706\u001b[0m     data \u001b[38;5;241m=\u001b[39m pandas_pa_type(data)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_numpy(np\u001b[38;5;241m.\u001b[39mfloat32, na_value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_pd_sparse_dtype(\u001b[38;5;28mgetattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, data)):\n\u001b[1;32m    711\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_dense()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:662\u001b[0m, in \u001b[0;36mIndexOpsMixin.to_numpy\u001b[0;34m(self, dtype, copy, na_value, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m         values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    660\u001b[0m     values[np\u001b[38;5;241m.\u001b[39masanyarray(isna(\u001b[38;5;28mself\u001b[39m))] \u001b[38;5;241m=\u001b[39m na_value\n\u001b[0;32m--> 662\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fillna) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mshares_memory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[:\u001b[38;5;241m2\u001b[39m], result[:\u001b[38;5;241m2\u001b[39m]):\n\u001b[1;32m    666\u001b[0m         \u001b[38;5;66;03m# Take slices to improve performance of check\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '-0.11%'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the train_model function using the sentiment and return data\n",
    "model_info = train_model(sentiment_data, return_data)\n",
    "print(\"Training complete!\")\n",
    "print(\"Model Information:\")\n",
    "print(model_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For demonstration, use the most recent date in the sentiment dataset as 'today'\n",
    "today_date = sentiment_data['Date'].max()\n",
    "print(\"Using sample date:\", today_date)\n",
    "\n",
    "# Extract sentiment data corresponding to the most recent date\n",
    "sentiment_today = sentiment_data[sentiment_data['Date'] == today_date].copy()\n",
    "\n",
    "# Construct a sample stock universe from the tickers present today\n",
    "stock_universe_today = sentiment_today['Ticker'].unique().tolist()\n",
    "print(\"Number of tickers in today's universe:\", len(stock_universe_today))\n",
    "\n",
    "# Generate predictions using the prediction function\n",
    "predictions = predict_returns(model_info, sentiment_today, stock_universe_today)\n",
    "print(\"Sample predictions:\")\n",
    "print(predictions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== Cell 9: Evaluation and Visualization ===\")\n",
    "# Re-engineer features for the training data (using the same function)\n",
    "features_train = engineer_features(sentiment_data, return_data, training=True)\n",
    "\n",
    "# Separate predictor variables (X) and target variable (y)\n",
    "X_train = features_train.drop(['Date', 'Ticker', 'Return'], axis=1)\n",
    "y_train = features_train['Return']\n",
    "\n",
    "# Fill missing values if any\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "\n",
    "# Scale features using the trained scaler from model_info\n",
    "X_train_scaled = model_info['scaler'].transform(X_train)\n",
    "\n",
    "# Generate predictions on the training set\n",
    "y_pred_train = model_info['model'].predict(X_train_scaled)\n",
    "\n",
    "# Plot Actual vs Predicted Returns\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_train, y_pred_train, alpha=0.5, label=\"Predictions\")\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', label=\"Ideal Fit\")\n",
    "plt.xlabel(\"Actual Return\")\n",
    "plt.ylabel(\"Predicted Return\")\n",
    "plt.title(\"Actual vs Predicted Returns on Training Data\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "print(f\"Training MSE: {mse_train:.6f}\")\n",
    "print(f\"Training MAE: {mae_train:.6f}\")\n",
    "print(f\"Training RÂ²: {r2_train:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Cell 10: Conclusions and Next Steps ===\")\n",
    "print(\"Key Insights:\")\n",
    "print(\"- The model has been trained using engineered sentiment features and shows moderate correlation between actual and predicted returns.\")\n",
    "print(\"- The scatter plot indicates how closely the model's predictions follow the ideal 45-degree line.\")\n",
    "print(\"\")\n",
    "print(\"Next Steps:\")\n",
    "print(\"1. Experiment with alternative models (e.g., Ridge, Lasso, ensemble methods).\")\n",
    "print(\"2. Enhance feature engineering by including additional temporal or interaction features.\")\n",
    "print(\"3. Evaluate the model on validation data with a time-based split to ensure robustness.\")\n",
    "print(\"4. Test the prediction function with live or out-of-sample data (2022-2024) for further analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
