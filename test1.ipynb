{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit data shape: (11929999, 21)\n",
      "Returns data shape: (2459589, 5)\n",
      "Reddit columns: ['StoryID', 'Post_Time', 'Received_Time', 'Ticker', 'Country', 'ISIN', 'Relevance', 'Sentiment', 'Confidence', 'Prob_POS', 'Prob_NTR', 'Prob_NEG', 'Reddit_Topic', 'TopicWeight', 'Alex_Topic', 'Source', 'SourceWeight', 'LinkID', 'Author', 'Novelty', 'Comment_Count']\n",
      "Returns columns: ['Date', 'Ticker', 'D0', 'D1', 'Return']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_32328\\1612327447.py:85: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  'Sentiment': ['mean', 'std', lambda x: stats.skew(x)],\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 5 elements, new values have 4 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 112\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m daily_agg\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Create features using the full reddit_df\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m features_df \u001b[38;5;241m=\u001b[39m create_features(reddit_df)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# ### Merge with Return Data\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# %% [code]\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Preprocess returns_df\u001b[39;00m\n\u001b[0;32m    122\u001b[0m returns_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(returns_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[1], line 92\u001b[0m, in \u001b[0;36mcreate_features\u001b[1;34m(df, rolling_windows, historical_df)\u001b[0m\n\u001b[0;32m     89\u001b[0m     agg_funcs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthor\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mnunique()\n\u001b[0;32m     91\u001b[0m daily_agg \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEffective_Date\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39magg(agg_funcs)\n\u001b[1;32m---> 92\u001b[0m daily_agg\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDaily_Sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment_Std\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment_Skew\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPost_Count\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     93\u001b[0m daily_agg \u001b[38;5;241m=\u001b[39m daily_agg\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Add log-transformed post count (avoid log(0) by adding 1)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6313\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   6311\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   6312\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[1;32m-> 6313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[0;32m   6314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m   6315\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mproperties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:814\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    813\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[1;32m--> 814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mset_axis(axis, labels)\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:238\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_set_axis(axis, new_labels)\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\base.py:98\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 5 elements, new values have 4 elements"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Sentiment-Based Return Prediction: Deliverable #2\n",
    "#\n",
    "# This notebook presents our approach to forecast next-day stock returns using Reddit sentiment data.\n",
    "#\n",
    "# We first perform extensive feature engineering on the raw sentiment data, then build a predictive model using\n",
    "# a regularized linear regression (Ridge) with time-series validation, and finally implement standardized prediction functions.\n",
    "#\n",
    "\n",
    "# %% [code]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# For preprocessing and modeling\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# For demonstration purposes, load training files (assume CSV files are in the same directory)\n",
    "returns_df = pd.read_csv('return_train_2017_2021.csv')\n",
    "reddit_df = pd.read_csv('sentiment_train_2017_2021.csv')\n",
    "\n",
    "print(\"Reddit data shape:\", reddit_df.shape)\n",
    "print(\"Returns data shape:\", returns_df.shape)\n",
    "print(\"Reddit columns:\", reddit_df.columns.tolist())\n",
    "print(\"Returns columns:\", returns_df.columns.tolist())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Data Preprocessing and Feature Engineering\n",
    "#\n",
    "# We start by converting time fields to Eastern Time and creating an effective date.\n",
    "#\n",
    "\n",
    "# %% [code]\n",
    "# Convert 'Received_Time' to datetime, localize to UTC, then convert to Eastern Time.\n",
    "reddit_df['Received_Time'] = pd.to_datetime(reddit_df['Received_Time'])\n",
    "reddit_df['Received_Time'] = reddit_df['Received_Time'].dt.tz_localize('UTC').dt.tz_convert('America/New_York')\n",
    "\n",
    "# Create an 'Effective_Date' column: for posts at or after 4 PM ET, shift the date to the next day.\n",
    "reddit_df['Effective_Date'] = reddit_df['Received_Time'].dt.date\n",
    "after_4pm = reddit_df['Received_Time'].dt.hour >= 16\n",
    "reddit_df.loc[after_4pm, 'Effective_Date'] = reddit_df.loc[after_4pm, 'Received_Time'].dt.date + pd.Timedelta(days=1)\n",
    "reddit_df['Effective_Date'] = pd.to_datetime(reddit_df['Effective_Date'])\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Feature Engineering\n",
    "#\n",
    "# The helper function below aggregates the raw sentiment data by ticker and day and then computes several features:\n",
    "#\n",
    "# 1. **Daily_Sentiment:** Mean sentiment of the day.\n",
    "# 2. **Sentiment_Std:** Standard deviation of sentiment.\n",
    "# 3. **Sentiment_Skew:** Skewness of the sentiment distribution.\n",
    "# 4. **Post_Count:** Total number of posts.\n",
    "# 5. **Log_Post_Count:** Log-transformed post count.\n",
    "# 6. **Unique_Author_Count:** Number of unique authors discussing the ticker.\n",
    "# 7. **Rolling_Avg_3D & Rolling_Avg_7D:** Three-day and seven-day rolling averages of the daily sentiment.\n",
    "#\n",
    "# If probability fields such as `Prob_POS` or `Prob_NEG` exist in the data, you can extend this function to incorporate them.\n",
    "#\n",
    "\n",
    "# %% [code]\n",
    "def create_features(df, rolling_windows=[3, 7], historical_df=None):\n",
    "    \"\"\"\n",
    "    Create aggregated sentiment features for each ticker and day.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Daily sentiment data with columns ['Ticker', 'Effective_Date', 'Sentiment'] and optionally 'Author'\n",
    "    rolling_windows : list of ints\n",
    "        Windows for rolling average computations.\n",
    "    historical_df : DataFrame or None\n",
    "        Optional historical data to compute rolling features if df covers a single day only.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    features_df : DataFrame\n",
    "        Aggregated feature DataFrame with one row per ticker per day.\n",
    "    \"\"\"\n",
    "    # Aggregate basic daily statistics for each ticker and day\n",
    "    agg_funcs = {\n",
    "        'Sentiment': ['mean', 'std', lambda x: stats.skew(x)],\n",
    "        'Ticker': 'count'  # Use count of rows as post count\n",
    "    }\n",
    "    if 'Author' in df.columns:\n",
    "        agg_funcs['Author'] = lambda x: x.nunique()\n",
    "    \n",
    "    daily_agg = df.groupby(['Ticker', 'Effective_Date']).agg(agg_funcs)\n",
    "    daily_agg.columns = ['Daily_Sentiment', 'Sentiment_Std', 'Sentiment_Skew', 'Post_Count']\n",
    "    daily_agg = daily_agg.reset_index()\n",
    "    \n",
    "    # Add log-transformed post count (avoid log(0) by adding 1)\n",
    "    daily_agg['Log_Post_Count'] = np.log(daily_agg['Post_Count'] + 1)\n",
    "    \n",
    "    if 'Author' in df.columns:\n",
    "        daily_agg.rename(columns={'Author': 'Unique_Author_Count'}, inplace=True)\n",
    "    \n",
    "    # Sort for rolling calculations\n",
    "    daily_agg = daily_agg.sort_values(by=['Ticker', 'Effective_Date'])\n",
    "    \n",
    "    # Compute rolling averages for each ticker using all available history\n",
    "    for window in rolling_windows:\n",
    "        col_name = f'Rolling_Avg_{window}D'\n",
    "        daily_agg[col_name] = daily_agg.groupby('Ticker')['Daily_Sentiment'].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "    \n",
    "    return daily_agg\n",
    "\n",
    "# Create features using the full reddit_df\n",
    "features_df = create_features(reddit_df)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Merge with Return Data\n",
    "#\n",
    "# Next, we merge our aggregated sentiment features with next-day stock returns.\n",
    "#\n",
    "\n",
    "# %% [code]\n",
    "# Preprocess returns_df\n",
    "returns_df['Date'] = pd.to_datetime(returns_df['Date'])\n",
    "if returns_df['Return'].dtype == object:\n",
    "    returns_df['Return'] = returns_df['Return'].str.rstrip('%').astype('float') / 100\n",
    "returns_df.dropna(inplace=True)\n",
    "# Shift date for alignment: signals generated on date d predict return on d+1.\n",
    "returns_df['Signal_Date'] = returns_df['Date'] - pd.Timedelta(days=1)\n",
    "\n",
    "# Merge on Ticker and date: note that our features df column is 'Effective_Date'\n",
    "merged_df = pd.merge(features_df, returns_df, left_on=['Ticker', 'Effective_Date'], right_on=['Ticker', 'Signal_Date'], how='inner')\n",
    "merged_df.drop(columns=['Signal_Date'], inplace=True)\n",
    "merged_df.rename(columns={'Return': 'Next_Day_Return', 'Effective_Date': 'Date'}, inplace=True)\n",
    "\n",
    "print(\"Merged data shape:\", merged_df.shape)\n",
    "merged_df.head()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Exploratory Analysis\n",
    "#\n",
    "# We can visualize the distribution of daily sentiment and next-day returns.\n",
    "#\n",
    "\n",
    "# %% [code]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(merged_df['Daily_Sentiment'], bins=30, alpha=0.7)\n",
    "plt.title('Distribution of Daily Sentiment')\n",
    "plt.xlabel('Daily Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(merged_df['Next_Day_Return'], bins=50, alpha=0.7)\n",
    "plt.title('Distribution of Next Day Returns')\n",
    "plt.xlabel('Next Day Return')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Model Development\n",
    "#\n",
    "# We split the training period (2017–2021) into training and validation subsets using a time-based split.\n",
    "# For demonstration, we use data up to the end of 2020 for training and reserve 2021 for validation.\n",
    "#\n",
    "\n",
    "# %% [code]\n",
    "# Time-based split: train on dates before 2021, validate on 2021 data\n",
    "train_df = merged_df[merged_df['Date'].dt.year < 2021].copy()\n",
    "val_df = merged_df[merged_df['Date'].dt.year == 2021].copy()\n",
    "\n",
    "# Select features\n",
    "feature_cols = ['Daily_Sentiment', 'Sentiment_Std', 'Sentiment_Skew', 'Post_Count', 'Log_Post_Count', \n",
    "                'Rolling_Avg_3D', 'Rolling_Avg_7D']\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['Next_Day_Return']\n",
    "\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df['Next_Day_Return']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Model Training using Ridge Regression\n",
    "#\n",
    "# We choose Ridge regression for its simplicity and robustness. We use time-series cross-validation to help avoid overfitting.\n",
    "#\n",
    "\n",
    "# %% [code]\n",
    "# Define candidate alphas for RidgeCV\n",
    "alphas = np.logspace(-3, 3, 13)\n",
    "ridge_model = RidgeCV(alphas=alphas, cv=TimeSeriesSplit(n_splits=5))\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Selected alpha:\", ridge_model.alpha_)\n",
    "\n",
    "# Evaluate on validation data\n",
    "y_val_pred = ridge_model.predict(X_val_scaled)\n",
    "mse = np.mean((y_val - y_val_pred) ** 2)\n",
    "mae = np.mean(np.abs(y_val - y_val_pred))\n",
    "r2 = ridge_model.score(X_val_scaled, y_val)\n",
    "\n",
    "print(f\"Validation Metrics -- MSE: {mse:.6f}, MAE: {mae:.6f}, R²: {r2:.4f}\")\n",
    "\n",
    "# Analyze feature importance (coefficients)\n",
    "coefficients = pd.Series(ridge_model.coef_, index=feature_cols)\n",
    "print(\"Feature Coefficients:\")\n",
    "print(coefficients.sort_values(ascending=False))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Standardized Prediction Function\n",
    "#\n",
    "# The following two functions implement the required standardized interface.\n",
    "#\n",
    "# 1. **train_model:** Trains a model using the full training dataset (sentiment and return data) and\n",
    "#    returns a dictionary containing the trained model, scaler, and metadata.\n",
    "#\n",
    "# 2. **predict_returns:** Uses the trained model to generate next-day return predictions for all stocks\n",
    "#    in the day’s trading universe. It accepts the current day’s sentiment data and a list of tickers,\n",
    "#    and it returns a DataFrame with columns: `Ticker`, `Predicted_Return`, and `Signal_Rank`.\n",
    "#\n",
    "\n",
    "# %% [code]\n",
    "def train_model(sentiment_data, return_data):\n",
    "    \"\"\"\n",
    "    Train a model using sentiment features to predict next-day returns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sentiment_data : DataFrame\n",
    "        The Reddit sentiment data for training (e.g. sentiment_train_2017_2021.csv)\n",
    "    return_data : DataFrame\n",
    "        The stock return data for training (e.g. return_train_2017_2021.csv)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model_info : dict\n",
    "        Dictionary containing the trained model object and necessary metadata.\n",
    "    \"\"\"\n",
    "    # --- Preprocessing: Convert dates and create effective date if needed ---\n",
    "    sentiment_data['Received_Time'] = pd.to_datetime(sentiment_data['Received_Time'])\n",
    "    sentiment_data['Received_Time'] = sentiment_data['Received_Time'].dt.tz_localize('UTC').dt.tz_convert('America/New_York')\n",
    "    sentiment_data['Effective_Date'] = sentiment_data['Received_Time'].dt.date\n",
    "    after_4pm = sentiment_data['Received_Time'].dt.hour >= 16\n",
    "    sentiment_data.loc[after_4pm, 'Effective_Date'] = sentiment_data.loc[after_4pm, 'Received_Time'].dt.date + pd.Timedelta(days=1)\n",
    "    sentiment_data['Effective_Date'] = pd.to_datetime(sentiment_data['Effective_Date'])\n",
    "    \n",
    "    # --- Feature Engineering ---\n",
    "    features = create_features(sentiment_data)\n",
    "    \n",
    "    # --- Merge with return data ---\n",
    "    return_data['Date'] = pd.to_datetime(return_data['Date'])\n",
    "    if return_data['Return'].dtype == object:\n",
    "        return_data['Return'] = return_data['Return'].str.rstrip('%').astype('float') / 100\n",
    "    return_data.dropna(inplace=True)\n",
    "    return_data['Signal_Date'] = return_data['Date'] - pd.Timedelta(days=1)\n",
    "    \n",
    "    merged = pd.merge(features, return_data, left_on=['Ticker', 'Effective_Date'], right_on=['Ticker', 'Signal_Date'], how='inner')\n",
    "    merged.drop(columns=['Signal_Date'], inplace=True)\n",
    "    merged.rename(columns={'Return': 'Next_Day_Return', 'Effective_Date': 'Date'}, inplace=True)\n",
    "    \n",
    "    # --- Split into training and validation subsets ---\n",
    "    # (For training the final model, you might use all available data; here we mimic a time-based split.)\n",
    "    train_mask = merged['Date'].dt.year < 2021\n",
    "    train_df = merged[train_mask].copy()\n",
    "    \n",
    "    feature_cols = ['Daily_Sentiment', 'Sentiment_Std', 'Sentiment_Skew', 'Post_Count', 'Log_Post_Count', \n",
    "                    'Rolling_Avg_3D', 'Rolling_Avg_7D']\n",
    "    X = train_df[feature_cols]\n",
    "    y = train_df['Next_Day_Return']\n",
    "    \n",
    "    scaler_local = StandardScaler()\n",
    "    X_scaled = scaler_local.fit_transform(X)\n",
    "    \n",
    "    # --- Model Training using Ridge Regression ---\n",
    "    alphas = np.logspace(-3, 3, 13)\n",
    "    model = RidgeCV(alphas=alphas, cv=TimeSeriesSplit(n_splits=5))\n",
    "    model.fit(X_scaled, y)\n",
    "    \n",
    "    # Save metadata necessary for out-of-sample prediction\n",
    "    model_info = {\n",
    "        \"model\": model,\n",
    "        \"scaler\": scaler_local,\n",
    "        \"feature_columns\": feature_cols,\n",
    "        \"feature_engineering_func\": create_features  # If needed for new data\n",
    "    }\n",
    "    \n",
    "    return model_info\n",
    "\n",
    "# %% [code]\n",
    "def predict_returns(model, sentiment_data_today, stock_universe_today, historical_data=None):\n",
    "    \"\"\"\n",
    "    Generate predictions of next-day returns for all stocks in the universe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : dict\n",
    "        Dictionary containing your trained model and metadata.\n",
    "    sentiment_data_today : DataFrame\n",
    "        Sentiment data for the current day.\n",
    "    stock_universe_today : list\n",
    "        List of stock tickers available for trading today.\n",
    "    historical_data : dict, optional\n",
    "        Dictionary containing historical sentiment and return data (if required for your model).\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : DataFrame\n",
    "        DataFrame with columns ['Ticker', 'Predicted_Return', 'Signal_Rank'].\n",
    "    \"\"\"\n",
    "    # Use the same feature engineering function to aggregate today’s data.\n",
    "    # If rolling features are required and historical data is provided, you would merge the historical data with today's.\n",
    "    # For simplicity, we assume that sentiment_data_today contains enough data for the day’s features.\n",
    "    features_today = model[\"feature_engineering_func\"](sentiment_data_today)\n",
    "    \n",
    "    # Create a DataFrame with one row per ticker in the universe.\n",
    "    # Merge the available features with the stock universe so that missing tickers are handled.\n",
    "    universe_df = pd.DataFrame({'Ticker': stock_universe_today})\n",
    "    pred_df = pd.merge(universe_df, features_today, on='Ticker', how='left')\n",
    "    \n",
    "    # For missing sentiment features, fill with default (e.g., 0).\n",
    "    feature_cols = model[\"feature_columns\"]\n",
    "    for col in feature_cols:\n",
    "        if col not in pred_df.columns:\n",
    "            pred_df[col] = 0\n",
    "    pred_df[feature_cols] = pred_df[feature_cols].fillna(0)\n",
    "    \n",
    "    # Scale features using the previously fitted scaler.\n",
    "    X_today = pred_df[feature_cols]\n",
    "    X_today_scaled = model[\"scaler\"].transform(X_today)\n",
    "    \n",
    "    # Predict next-day returns.\n",
    "    pred_df['Predicted_Return'] = model[\"model\"].predict(X_today_scaled)\n",
    "    \n",
    "    # Calculate the percentile rank (signal rank) for each ticker.\n",
    "    pred_df['Signal_Rank'] = pred_df['Predicted_Return'].rank(pct=True)\n",
    "    \n",
    "    # Prepare final output with required columns.\n",
    "    predictions = pred_df[['Ticker', 'Predicted_Return', 'Signal_Rank']]\n",
    "    return predictions\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Example Usage\n",
    "#\n",
    "# Once you have trained the model using historical data (e.g., 2017-2021), you can generate daily predictions as shown below.\n",
    "#\n",
    "\n",
    "# %% [code]\n",
    "# Train the model\n",
    "model_info = train_model(reddit_df, returns_df)\n",
    "\n",
    "# For demonstration, assume sentiment_data_today is a subset of reddit_df for a recent day\n",
    "# (In practice, you would use the actual current day data.)\n",
    "today_date = reddit_df['Effective_Date'].max()\n",
    "sentiment_today = reddit_df[reddit_df['Effective_Date'] == today_date].copy()\n",
    "\n",
    "# Assume the stock universe today is the unique tickers from sentiment_today and possibly additional ones.\n",
    "stock_universe = list(set(sentiment_today['Ticker'].unique()) | set(returns_df['Ticker'].unique()))\n",
    "\n",
    "# Generate predictions\n",
    "predictions = predict_returns(model_info, sentiment_today, stock_universe)\n",
    "print(predictions.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
