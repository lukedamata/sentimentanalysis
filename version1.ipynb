{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326e8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_28224\\3288945052.py:35: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return stats.skew(x) if len(x) > 2 else 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in merged data: 0\n",
      "Validation MSE: 0.001863\n",
      "Validation MAE: 0.023118\n",
      "Validation R²: -0.0007\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Load data\n",
    "returns_df = pd.read_csv('return_train_2017_2021.csv')\n",
    "reddit_df = pd.read_csv('sentiment_train_2017_2021.csv')\n",
    "\n",
    "# Data cleaning functions\n",
    "def clean_returns_data(df):\n",
    "    df = df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    if df['Return'].dtype == object:\n",
    "        df['Return'] = df['Return'].str.rstrip('%').astype('float') / 100\n",
    "    df = df.dropna(subset=['Return'])\n",
    "    return df\n",
    "\n",
    "def clean_sentiment_data(df):\n",
    "    df = df.copy()\n",
    "    df['Received_Time'] = pd.to_datetime(df['Received_Time'])\n",
    "    df['Received_Time'] = df['Received_Time'].dt.tz_localize('UTC').dt.tz_convert('America/New_York')\n",
    "    df['Effective_Date'] = df['Received_Time'].dt.floor('D')\n",
    "    after_4pm = df['Received_Time'].dt.hour >= 16\n",
    "    df.loc[after_4pm, 'Effective_Date'] += pd.Timedelta(days=1)\n",
    "    return df\n",
    "\n",
    "# Clean data\n",
    "returns_df = clean_returns_data(returns_df)\n",
    "reddit_df = clean_sentiment_data(reddit_df)\n",
    "\n",
    "def calculate_skew(x):\n",
    "    return stats.skew(x) if len(x) > 2 else 0\n",
    "\n",
    "def create_features(df, rolling_windows=[3, 7]):\n",
    "    \"\"\"Create features with robust NaN handling\"\"\"\n",
    "    # Basic aggregations\n",
    "    agg_dict = {\n",
    "        'Sentiment': ['mean', 'std', calculate_skew],\n",
    "        'Ticker': 'count'\n",
    "    }\n",
    "    \n",
    "    if 'Author' in df.columns:\n",
    "        agg_dict['Author'] = 'nunique'\n",
    "    \n",
    "    # Group and aggregate\n",
    "    daily_agg = df.groupby(['Ticker', 'Effective_Date']).agg(agg_dict)\n",
    "    daily_agg.columns = [f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) else col for col in daily_agg.columns]\n",
    "    daily_agg = daily_agg.reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    rename_map = {\n",
    "        'Sentiment_mean': 'Daily_Sentiment',\n",
    "        'Sentiment_std': 'Sentiment_Std',\n",
    "        'Sentiment_calculate_skew': 'Sentiment_Skew',\n",
    "        'Ticker_count': 'Post_Count'\n",
    "    }\n",
    "    if 'Author_nunique' in daily_agg.columns:\n",
    "        rename_map['Author_nunique'] = 'Unique_Author_Count'\n",
    "    \n",
    "    daily_agg = daily_agg.rename(columns=rename_map)\n",
    "    \n",
    "    # Ensure required columns exist and fill NAs\n",
    "    required = ['Daily_Sentiment', 'Sentiment_Std', 'Sentiment_Skew', 'Post_Count']\n",
    "    for col in required:\n",
    "        if col not in daily_agg.columns:\n",
    "            daily_agg[col] = 0\n",
    "        daily_agg[col] = daily_agg[col].fillna(0)\n",
    "    \n",
    "    # Add derived features\n",
    "    daily_agg['Log_Post_Count'] = np.log1p(daily_agg['Post_Count'])\n",
    "    daily_agg = daily_agg.sort_values(['Ticker', 'Effective_Date'])\n",
    "    \n",
    "    # Rolling features with min_periods=1 to avoid NaNs\n",
    "    for window in rolling_windows:\n",
    "        col_name = f'Rolling_Avg_{window}D'\n",
    "        daily_agg[col_name] = daily_agg.groupby('Ticker')['Daily_Sentiment'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    return daily_agg\n",
    "\n",
    "# Create features with NaN handling\n",
    "features_df = create_features(reddit_df)\n",
    "\n",
    "# Prepare for merging\n",
    "returns_df['Signal_Date'] = returns_df['Date'] - pd.Timedelta(days=1)\n",
    "features_df['Date_Key'] = features_df['Effective_Date'].dt.strftime('%Y-%m-%d')\n",
    "returns_df['Signal_Date_Key'] = returns_df['Signal_Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Merge with inner join to avoid NaN targets\n",
    "# ... [your code up through the merge] ...\n",
    "\n",
    "merged_df = (\n",
    "    pd.merge(\n",
    "        features_df,\n",
    "        returns_df,\n",
    "        left_on=['Ticker','Date_Key'],\n",
    "        right_on=['Ticker','Signal_Date_Key'],\n",
    "        how='inner'\n",
    "    )\n",
    "    .drop(columns=['Date_Key','Signal_Date_Key','Signal_Date'])\n",
    "    .rename(columns={'Return':'Next_Day_Return'})\n",
    ")\n",
    "\n",
    "print(\"NaN values in merged data:\", merged_df.isna().sum().sum())\n",
    "\n",
    "# split\n",
    "train_df = merged_df[merged_df['Date'].dt.year < 2021].copy()\n",
    "val_df   = merged_df[merged_df['Date'].dt.year == 2021].copy()\n",
    "\n",
    "# prepare X and y\n",
    "X_train = train_df[required_cols].fillna(0)\n",
    "y_train = train_df['Next_Day_Return'].fillna(0)\n",
    "X_val   = val_df  [required_cols].fillna(0)\n",
    "y_val   = val_df  ['Next_Day_Return'].fillna(0)\n",
    "\n",
    "# ... rest of your scaling, training, evaluation ...\n",
    "\n",
    "\n",
    "# Verify no NaNs\n",
    "assert not X_train.isna().any().any(), \"Training features contain NaNs\"\n",
    "assert not y_train.isna().any(), \"Training targets contain NaNs\"\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Train model\n",
    "ridge_model = RidgeCV(alphas=np.logspace(-3, 3, 13), cv=TimeSeriesSplit(n_splits=5))\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_val_pred = ridge_model.predict(X_val_scaled)\n",
    "print(f\"Validation MSE: {np.mean((y_val - y_val_pred)**2):.6f}\")\n",
    "print(f\"Validation MAE: {np.mean(np.abs(y_val - y_val_pred)):.6f}\")\n",
    "print(f\"Validation R²: {ridge_model.score(X_val_scaled, y_val):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
