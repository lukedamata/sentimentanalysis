{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0171d7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_58824\\3401155447.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ws = df.groupby(['Ticker','Date']).apply(\n",
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_58824\\3401155447.py:76: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  agg = agg.groupby('Ticker').apply(ts_feats).reset_index(drop=True)\n",
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_58824\\3401155447.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ws = df.groupby(['Ticker','Date']).apply(\n",
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_58824\\3401155447.py:76: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  agg = agg.groupby('Ticker').apply(ts_feats).reset_index(drop=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_58824\\3401155447.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0msentiment_data_today\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessed_sentiment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreprocessed_sentiment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msample_day\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0mstock_universe_today\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreturn_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msample_day\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Ticker'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_returns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentiment_data_today\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentiment_data_today\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstock_universe_today\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstock_universe_today\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sample predictions:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_58824\\3401155447.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, sentiment_data_today, stock_universe_today)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mft\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mft\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Ticker'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mft\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Ticker'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m             \u001b[0mft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Ticker'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Date'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtoday\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;31m# Ensure all feature columns exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feature_columns'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         ):\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "001_prediction_function.py\n",
    "\n",
    "Final deliverable with:\n",
    "- Time-series cross-validation\n",
    "- Stronger regularization (dropout, weight decay, early stopping)\n",
    "- Expanded feature set (including volatility measures)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from datetime import time, timedelta\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def convert_return(x):\n",
    "    if isinstance(x, str) and x.endswith('%'):\n",
    "        return float(x[:-1].strip())/100\n",
    "    return float(x)\n",
    "\n",
    "def preprocess_sentiment_data(df):\n",
    "    df = df.copy()\n",
    "    df['Received_Time'] = pd.to_datetime(df['Received_Time'], utc=True)\n",
    "    df['Received_Time_EST'] = df['Received_Time'].dt.tz_convert('America/New_York')\n",
    "    cutoff = time(16,0)\n",
    "    df['Date'] = df['Received_Time_EST'].apply(\n",
    "        lambda x: pd.to_datetime(x.date()+timedelta(days=1))\n",
    "        if x.time()>cutoff else pd.to_datetime(x.date())\n",
    "    )\n",
    "    df['Ticker'] = df['Ticker'].str.upper()\n",
    "    return df\n",
    "\n",
    "def create_features(df):\n",
    "    # basic and net sentiment\n",
    "    agg = df.groupby(['Ticker','Date']).agg(\n",
    "        sentiment_mean=('Sentiment','mean'),\n",
    "        sentiment_std=('Sentiment','std'),\n",
    "        post_count=('Sentiment','count'),\n",
    "        net_sentiment=('Sentiment','sum'),\n",
    "        avg_confidence=('Confidence','mean'),\n",
    "        avg_prob_pos=('Prob_POS','mean'),\n",
    "        avg_prob_ntr=('Prob_NTR','mean'),\n",
    "        avg_prob_neg=('Prob_NEG','mean'),\n",
    "        avg_source_weight=('SourceWeight','mean'),\n",
    "        avg_topic_weight=('TopicWeight','mean'),\n",
    "        avg_relevance=('Relevance','mean'),\n",
    "    ).fillna(0).reset_index()\n",
    "\n",
    "    # weighted sentiment\n",
    "    ws = df.groupby(['Ticker','Date']).apply(\n",
    "        lambda d: (d.Sentiment*d.Relevance).sum()/d.Sentiment.count()\n",
    "        if d.Sentiment.count()>0 else 0\n",
    "    ).reset_index(name='weighted_sentiment')\n",
    "    agg = agg.merge(ws, on=['Ticker','Date'], how='left')\n",
    "\n",
    "    # time-series per ticker\n",
    "    def ts_feats(g):\n",
    "        g = g.sort_values('Date').copy()\n",
    "        g['cum_sent'] = g.net_sentiment.cumsum()\n",
    "        g['daily_change'] = g.net_sentiment.diff().fillna(0)\n",
    "        g['ma_5'] = g.net_sentiment.rolling(5,min_periods=1).mean()\n",
    "        g['ma_10'] = g.net_sentiment.rolling(10,min_periods=1).mean()\n",
    "        g['past3'] = g.net_sentiment.shift(1).rolling(3,min_periods=1).sum().fillna(0)\n",
    "        g['log_vol'] = np.log1p(g.post_count)\n",
    "        # new volatility features\n",
    "        g['vol_5'] = g.net_sentiment.rolling(5,min_periods=1).std().fillna(0)\n",
    "        g['range_5'] = (g.net_sentiment.rolling(5,min_periods=1).max() - \n",
    "                        g.net_sentiment.rolling(5,min_periods=1).min()).fillna(0)\n",
    "        return g\n",
    "\n",
    "    agg = agg.groupby('Ticker').apply(ts_feats).reset_index(drop=True)\n",
    "\n",
    "    # topic/source counts (abbreviated hereâ€”expand to full list as before)\n",
    "    topics = [t.upper() for t in [\n",
    "        \"Biotech\",\"Chart\",\"Commentary\",\"Daily Discussion\",\"Daily Thread\",\"DD\",\"Discussion\",\n",
    "        \"Distressed\",\"Earnings Thread\",\"Education\",\"Energy\",\"Fundamentals\",\"Futures\",\"Gain\",\n",
    "        \"Help\",\"Industry Report\",\"Interview/Profile\",\"Investor Letter\",\"Long Thesis\",\"Loss\",\n",
    "        \"Macro\",\"Meme\",\"Mods\",\"News\",\"None\",\"Options\",\"Profit\",\"Question\",\"Retail\",\"Satire\",\n",
    "        \"Shitpost\",\"Short Thesis\",\"Special Situation\",\"Stocks\",\"Storytime\",\"Strategy\",\n",
    "        \"TAG ME PLS\",\"Technicals\",\"Thesis\",\"WALL ST. \\\"LEAKS\\\"\",\"Weekend Discussion\",\n",
    "        \"WSBbooks\",\"YOLO\"\n",
    "    ]]\n",
    "    for src in ['WSB','INVESTING']:\n",
    "        mask = df.Source.str.upper().str.contains(src, na=False)\n",
    "        sub = df[mask]\n",
    "        cnt = sub.groupby(['Ticker','Date'])['Reddit_Topic']\\\n",
    "                 .apply(lambda s: s.str.upper().isin(topics).value_counts())\\\n",
    "                 .unstack(fill_value=0)\n",
    "        for t in topics:\n",
    "            col = f\"{src}_count_{t}\"\n",
    "            cnt[col] = cnt.get(t,0)\n",
    "        cnt = cnt[[f\"{src}_count_{t}\" for t in topics]].reset_index()\n",
    "        agg = agg.merge(cnt, on=['Ticker','Date'], how='left').fillna(0)\n",
    "\n",
    "    return agg\n",
    "\n",
    "class FeedforwardNet(nn.Module):\n",
    "    def __init__(self, in_dim, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "def train_model(sentiment_data, return_data):\n",
    "    sd = preprocess_sentiment_data(sentiment_data)\n",
    "    feats = create_features(sd)\n",
    "    rd = return_data.copy()\n",
    "    rd['Date']=pd.to_datetime(rd['Date']).dt.normalize()\n",
    "    rd['Ticker']=rd['Ticker'].str.upper()\n",
    "    rd['Return']=rd['Return'].apply(convert_return)\n",
    "    data = feats.merge(rd[['Date','Ticker','Return']],on=['Date','Ticker']).dropna(subset=['Return'])\n",
    "    data = data.sort_values('Date')\n",
    "    X = data.drop(['Date','Ticker','Return'],axis=1).values\n",
    "    y = data['Return'].values.reshape(-1,1)\n",
    "\n",
    "    # cross-validation splits\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    best_model, best_scaler, best_loss = None, None, np.inf\n",
    "\n",
    "    for train_idx,val_idx in tscv.split(X):\n",
    "        X_tr, X_va = X[train_idx], X[val_idx]\n",
    "        y_tr, y_va = y[train_idx], y[val_idx]\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_s, X_va_s = scaler.transform(X_tr), scaler.transform(X_va)\n",
    "\n",
    "        # tensors\n",
    "        Xt = torch.tensor(X_tr_s,dtype=torch.float32).to(device)\n",
    "        yt = torch.tensor(y_tr,dtype=torch.float32).to(device)\n",
    "        Xv = torch.tensor(X_va_s,dtype=torch.float32).to(device)\n",
    "        yv = torch.tensor(y_va,dtype=torch.float32).to(device)\n",
    "\n",
    "        model = FeedforwardNet(Xt.shape[1],drop=0.4).to(device)\n",
    "        opt = optim.AdamW(model.parameters(),lr=5e-4,weight_decay=1e-3)\n",
    "        crit = nn.MSELoss()\n",
    "\n",
    "        # early stopping\n",
    "        es_best, es_cnt = np.inf, 0\n",
    "        for ep in range(200):\n",
    "            model.train(); opt.zero_grad()\n",
    "            loss = crit(model(Xt),yt)\n",
    "            loss.backward(); opt.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                vloss = crit(model(Xv),yv).item()\n",
    "            if vloss < es_best:\n",
    "                es_best, es_cnt = vloss, 0\n",
    "                best_state = model.state_dict()\n",
    "            else:\n",
    "                es_cnt += 1\n",
    "                if es_cnt>=20: break\n",
    "\n",
    "        model.load_state_dict(best_state)\n",
    "        if es_best < best_loss:\n",
    "            best_loss, best_model, best_scaler = es_best, model, scaler\n",
    "\n",
    "    # final model on full data\n",
    "    X_s = best_scaler.transform(X)\n",
    "    Xt_all = torch.tensor(X_s,dtype=torch.float32).to(device)\n",
    "    yt_all = torch.tensor(y,dtype=torch.float32).to(device)\n",
    "    final = FeedforwardNet(X_s.shape[1],drop=0.4).to(device)\n",
    "    final.load_state_dict(best_model.state_dict())\n",
    "    opt = optim.AdamW(final.parameters(),lr=2e-4,weight_decay=1e-3)\n",
    "    for ep in range(50):\n",
    "        final.train(); opt.zero_grad()\n",
    "        crit(final(Xt_all),yt_all).backward(); opt.step()\n",
    "\n",
    "    return {\n",
    "        'model': final,\n",
    "        'scaler': best_scaler,\n",
    "        'feature_columns': data.drop(['Date','Ticker','Return'],axis=1).columns.tolist(),\n",
    "        'device': device\n",
    "    }\n",
    "\n",
    "def predict_returns(model, sentiment_data_today, stock_universe_today):\n",
    "    \"\"\"\n",
    "    Generate predictions of next-day returns for all stocks in the universe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : dict\n",
    "        Contains the trained model, scaler, feature columns, and device info.\n",
    "    sentiment_data_today : DataFrame\n",
    "        New sentiment data (for a single day).\n",
    "    stock_universe_today : list\n",
    "        List of stock tickers available today.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predictions : DataFrame\n",
    "        A DataFrame with columns ['Ticker', 'Predicted_Return', 'Signal_Rank'].\n",
    "    \"\"\"\n",
    "    # Preprocess today's sentiment data\n",
    "    sd = preprocess_sentiment_data(sentiment_data_today)\n",
    "    \n",
    "    # Determine current date\n",
    "    if sd.empty:\n",
    "        today = pd.Timestamp.today().normalize()\n",
    "    else:\n",
    "        today = sd['Date'].max()\n",
    "    \n",
    "    # Create features up to today\n",
    "    feats = create_features(sd[sd['Date'] <= today])\n",
    "    \n",
    "    # Filter or fill for the universe\n",
    "    ft = feats[feats['Date'] == today].copy()\n",
    "    tickers = [t.upper() for t in stock_universe_today]\n",
    "    \n",
    "    if ft.empty or 'Ticker' not in ft.columns:\n",
    "        ft = pd.DataFrame({'Ticker': tickers, 'Date': today})\n",
    "    else:\n",
    "        ft = ft[ft['Ticker'].isin(tickers)]\n",
    "        missing = set(tickers) - set(ft['Ticker'])\n",
    "        for m in missing:\n",
    "            ft = ft.append({'Ticker': m, 'Date': today}, ignore_index=True)\n",
    "    \n",
    "    # Ensure all feature columns exist\n",
    "    for col in model['feature_columns']:\n",
    "        if col not in ft.columns:\n",
    "            ft[col] = 0\n",
    "    \n",
    "    ft = ft.sort_values('Ticker').reset_index(drop=True)\n",
    "    \n",
    "    # Scale and predict\n",
    "    X = ft[model['feature_columns']].fillna(0).values\n",
    "    Xs = model['scaler'].transform(X)\n",
    "    Xt = torch.tensor(Xs, dtype=torch.float32).to(model['device'])\n",
    "    \n",
    "    net = model['model']\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = net(Xt).cpu().numpy().flatten()\n",
    "    \n",
    "    # Break ties and rank\n",
    "    preds += np.random.normal(0, 1e-6, size=preds.shape)\n",
    "    out = pd.DataFrame({\n",
    "        'Ticker': ft['Ticker'],\n",
    "        'Predicted_Return': preds\n",
    "    })\n",
    "    out['Signal_Rank'] = out['Predicted_Return'].rank(pct=True)\n",
    "    \n",
    "    return out[['Ticker', 'Predicted_Return', 'Signal_Rank']]\n",
    "\n",
    "####################################\n",
    "# Test Section (Runs if the script is executed directly)\n",
    "####################################\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        sentiment_data = pd.read_csv('./data/sentiment_train_2017_2021.csv')\n",
    "        return_data = pd.read_csv('./data/return_train_2017_2021.csv')\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Data files not found. Please check file paths.\")\n",
    "        sentiment_data = None\n",
    "        return_data = None\n",
    "\n",
    "    if sentiment_data is not None and return_data is not None:\n",
    "        # Normalize the return_data Date column.\n",
    "        return_data['Date'] = pd.to_datetime(return_data['Date']).dt.normalize()\n",
    "        \n",
    "        model = train_model(sentiment_data, return_data)\n",
    "        \n",
    "        sample_day = pd.to_datetime('2021-06-01').normalize()\n",
    "        preprocessed_sentiment = preprocess_sentiment_data(sentiment_data)\n",
    "        sentiment_data_today = preprocessed_sentiment[preprocessed_sentiment['Date'] == sample_day].copy()\n",
    "        \n",
    "        stock_universe_today = return_data[return_data['Date'] == sample_day]['Ticker'].unique().tolist()\n",
    "        \n",
    "        predictions = predict_returns(model, sentiment_data_today=sentiment_data_today, stock_universe_today=stock_universe_today)\n",
    "        print(\"Sample predictions:\")\n",
    "        print(predictions.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
