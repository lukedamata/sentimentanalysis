{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509f446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Preprocessing sentiment data...\n",
      "Creating features from sentiment data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_44800\\1749845377.py:189: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  special_aggs = df.groupby(['Ticker', 'Date']).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging sentiment features with stock returns...\n",
      "Training fold 214 days -> 209 days...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_44800\\1749845377.py:367: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Scalar.cpp:23.)\n",
      "  print(f\"Epoch {epoch:3d} | Train Loss: {loss.item():.6f} | Val Loss: {val_loss.item():.6f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 0.052968 | Val Loss: 0.018206\n",
      "Epoch  20 | Train Loss: 0.005217 | Val Loss: 0.002273\n",
      "Epoch  40 | Train Loss: 0.002580 | Val Loss: 0.001560\n",
      "Epoch  60 | Train Loss: 0.001950 | Val Loss: 0.001304\n",
      "Epoch  80 | Train Loss: 0.001621 | Val Loss: 0.001347\n",
      "Epoch 100 | Train Loss: 0.001491 | Val Loss: 0.001277\n",
      "Epoch 120 | Train Loss: 0.001368 | Val Loss: 0.001237\n",
      "Epoch 140 | Train Loss: 0.001287 | Val Loss: 0.001206\n",
      "Epoch 160 | Train Loss: 0.001228 | Val Loss: 0.001165\n",
      "Epoch 180 | Train Loss: 0.001136 | Val Loss: 0.001134\n",
      "Epoch 200 | Train Loss: 0.001079 | Val Loss: 0.001106\n",
      "Epoch 220 | Train Loss: 0.001035 | Val Loss: 0.001074\n",
      "Epoch 240 | Train Loss: 0.000997 | Val Loss: 0.001052\n",
      "Epoch 260 | Train Loss: 0.000964 | Val Loss: 0.001035\n",
      "Epoch 280 | Train Loss: 0.000926 | Val Loss: 0.001009\n",
      "Fold final validation loss: 0.000998\n",
      "Training fold 423 days -> 209 days...\n",
      "Epoch   0 | Train Loss: 0.015436 | Val Loss: 0.004847\n",
      "Epoch  20 | Train Loss: 0.002240 | Val Loss: 0.001263\n",
      "Epoch  40 | Train Loss: 0.001169 | Val Loss: 0.001108\n",
      "Epoch  60 | Train Loss: 0.000938 | Val Loss: 0.001029\n",
      "Epoch  80 | Train Loss: 0.000855 | Val Loss: 0.000994\n",
      "Epoch 100 | Train Loss: 0.000801 | Val Loss: 0.000967\n",
      "Epoch 120 | Train Loss: 0.000767 | Val Loss: 0.000954\n",
      "Epoch 140 | Train Loss: 0.000752 | Val Loss: 0.000943\n",
      "Epoch 160 | Train Loss: 0.000734 | Val Loss: 0.000937\n",
      "Epoch 180 | Train Loss: 0.000726 | Val Loss: 0.000933\n",
      "Epoch 200 | Train Loss: 0.000723 | Val Loss: 0.000931\n",
      "Epoch 220 | Train Loss: 0.000713 | Val Loss: 0.000929\n",
      "Epoch 240 | Train Loss: 0.000712 | Val Loss: 0.000928\n",
      "Epoch 260 | Train Loss: 0.000708 | Val Loss: 0.000927\n",
      "Epoch 280 | Train Loss: 0.000704 | Val Loss: 0.000927\n",
      "Fold final validation loss: 0.000926\n",
      "Training fold 632 days -> 209 days...\n",
      "Epoch   0 | Train Loss: 0.035036 | Val Loss: 0.010391\n",
      "Epoch  20 | Train Loss: 0.004972 | Val Loss: 0.003780\n",
      "Epoch  40 | Train Loss: 0.002767 | Val Loss: 0.003931\n",
      "Epoch  60 | Train Loss: 0.002040 | Val Loss: 0.003722\n",
      "Early stopping at epoch 79\n",
      "Fold final validation loss: 0.003531\n",
      "Training fold 841 days -> 209 days...\n",
      "Epoch   0 | Train Loss: 0.013486 | Val Loss: 0.004665\n",
      "Epoch  20 | Train Loss: 0.002691 | Val Loss: 0.003146\n",
      "Epoch  40 | Train Loss: 0.002001 | Val Loss: 0.003430\n",
      "Early stopping at epoch 43\n",
      "Fold final validation loss: 0.003310\n",
      "Training fold 1050 days -> 209 days...\n",
      "Epoch   0 | Train Loss: 0.014629 | Val Loss: 0.004284\n",
      "Epoch  20 | Train Loss: 0.002565 | Val Loss: 0.002140\n",
      "Epoch  40 | Train Loss: 0.001968 | Val Loss: 0.001991\n",
      "Epoch  60 | Train Loss: 0.001849 | Val Loss: 0.001929\n",
      "Epoch  80 | Train Loss: 0.001804 | Val Loss: 0.001906\n",
      "Epoch 100 | Train Loss: 0.001783 | Val Loss: 0.001898\n",
      "Epoch 120 | Train Loss: 0.001779 | Val Loss: 0.001895\n",
      "Epoch 140 | Train Loss: 0.001766 | Val Loss: 0.001894\n",
      "Epoch 160 | Train Loss: 0.001762 | Val Loss: 0.001893\n",
      "Epoch 180 | Train Loss: 0.001762 | Val Loss: 0.001893\n",
      "Epoch 200 | Train Loss: 0.001756 | Val Loss: 0.001893\n",
      "Early stopping at epoch 211\n",
      "Fold final validation loss: 0.001893\n",
      "Training final model on complete dataset...\n",
      "Final model - Epoch   0 | Loss: 0.002323\n",
      "Final model - Epoch  20 | Loss: 0.002083\n",
      "Final model - Epoch  40 | Loss: 0.001812\n",
      "Final model - Epoch  60 | Loss: 0.001804\n",
      "Final model - Epoch  80 | Loss: 0.001794\n",
      "\n",
      "Top 10 most important features:\n",
      "1. unique_author_count: 0.043802\n",
      "2. total_relevance: 0.039886\n",
      "3. post_count: 0.039586\n",
      "4. sentiment_median: 0.021340\n",
      "5. log_post_count: 0.021203\n",
      "6. max_prob_pos: 0.019547\n",
      "7. sentiment_mean: 0.019414\n",
      "8. avg_prob_neg: 0.019219\n",
      "9. intraday_sentiment_volatility: 0.017739\n",
      "10. author_avg_sentiment: 0.017565\n",
      "Training complete. Model is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_44800\\1749845377.py:189: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  special_aggs = df.groupby(['Ticker', 'Date']).apply(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Ticker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44800\\1749845377.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0msentiment_data_today\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessed_sentiment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreprocessed_sentiment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msample_day\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[0mstock_universe_today\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreturn_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msample_day\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Ticker'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_returns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentiment_data_today\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstock_universe_today\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sample predictions:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44800\\1749845377.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model_info, sentiment_data_today, stock_universe_today, historical_data)\u001b[0m\n\u001b[0;32m    534\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feature_columns'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatures_today\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m             \u001b[0mfeatures_today\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[0mfeatures_today\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures_today\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Ticker'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[1;31m# Prepare features for prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[0mX_today\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures_today\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feature_columns'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   7185\u001b[0m             )\n\u001b[0;32m   7186\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7187\u001b[0m             \u001b[1;31m# len(by) == 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7189\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7191\u001b[0m             \u001b[1;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7192\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Ticker'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "001_prediction_function.py\n",
    "\n",
    "This file provides two main functions:\n",
    "    1. train_model(sentiment_data, return_data)\n",
    "    2. predict_returns(model_info, sentiment_data_today, stock_universe_today, historical_data=None)\n",
    "\n",
    "The code processes Reddit sentiment data, creates aggregated features, and trains a PyTorch feedforward neural network (with GPU support when available)\n",
    "to predict next-day stock returns.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime, time, timedelta\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import os\n",
    "\n",
    "# Set device for GPU usage if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "####################################\n",
    "# Helper Functions\n",
    "####################################\n",
    "\n",
    "def convert_return(x):\n",
    "    \"\"\"\n",
    "    Converts a return value to a float.\n",
    "    If x is a string ending in '%', removes the '%' and divides by 100.\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if x.endswith('%'):\n",
    "            return float(x[:-1].strip()) / 100.0\n",
    "        else:\n",
    "            return float(x)\n",
    "    else:\n",
    "        return float(x)\n",
    "\n",
    "def preprocess_sentiment_data(sentiment_data):\n",
    "    \"\"\"\n",
    "    Preprocess sentiment data:\n",
    "      - Convert 'Received_Time' to a timezone-aware datetime (UTC) then to US/Eastern.\n",
    "      - Create a 'Date' column by shifting posts received after 4:00 PM (EST) to the next day.\n",
    "      - Ensure 'Ticker' is uppercase.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentiment_data : DataFrame\n",
    "        Raw sentiment data containing at least 'Received_Time' and 'Ticker'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        Processed sentiment data with added 'Received_Time_EST' and 'Date' columns.\n",
    "    \"\"\"\n",
    "    df = sentiment_data.copy()\n",
    "    if 'Received_Time' not in df.columns:\n",
    "        raise ValueError(\"Column 'Received_Time' not found in the sentiment data.\")\n",
    "    \n",
    "    df['Received_Time'] = pd.to_datetime(df['Received_Time'], utc=True)\n",
    "    df['Received_Time_EST'] = df['Received_Time'].dt.tz_convert('America/New_York')\n",
    "    df['local_date'] = df['Received_Time_EST'].dt.date\n",
    "    cutoff = time(16, 0)  # 4:00 PM cutoff\n",
    "    df['Date'] = df['Received_Time_EST'].apply(\n",
    "        lambda x: pd.to_datetime(x.date() + timedelta(days=1)) if x.time() > cutoff else pd.to_datetime(x.date())\n",
    "    )\n",
    "    if 'Ticker' in df.columns:\n",
    "        df['Ticker'] = df['Ticker'].str.upper()\n",
    "    return df\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create advanced daily features by aggregating sentiment data for each Ticker and Date.\n",
    "    \n",
    "    Features include:\n",
    "      - Basic statistics: mean, std, count\n",
    "      - Advanced sentiment metrics: weighted sentiment, sentiment skewness\n",
    "      - Volume indicators: log_post_count, normalized_post_volume\n",
    "      - Temporal patterns: am_pm_sentiment_ratio, intraday_sentiment_volatility\n",
    "      - Probability-based measures: sentiment_confidence_product, pos_neg_ratio\n",
    "      - Source and topic relevance: relevance_weighted_sentiment\n",
    "      - Author-based metrics: unique_author_count, avg_author_sentiment\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Preprocessed sentiment data with columns including \n",
    "        ['Ticker', 'Date', 'Sentiment', 'Confidence', 'Prob_POS', 'Prob_NTR', 'Prob_NEG',\n",
    "         'Relevance', 'SourceWeight', 'TopicWeight', 'Author'].\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    features : DataFrame\n",
    "        Aggregated features for each Ticker and Date.\n",
    "    \"\"\"\n",
    "    # If the dataframe is empty, return an empty dataframe with all required columns\n",
    "    if df.empty:\n",
    "        empty_df = pd.DataFrame(columns=['Ticker', 'Date', 'sentiment_mean', 'sentiment_std', 'post_count', \n",
    "                                         'sentiment_skew', 'sentiment_median', 'avg_confidence', 'confidence_std',\n",
    "                                         'avg_prob_pos', 'max_prob_pos', 'avg_prob_ntr', 'avg_prob_neg', \n",
    "                                         'max_prob_neg', 'avg_source_weight', 'avg_topic_weight', 'avg_relevance',\n",
    "                                         'total_relevance', 'unique_author_count', 'morning_post_ratio',\n",
    "                                         'market_hours_ratio', 'log_post_count', 'pos_neg_ratio',\n",
    "                                         'sentiment_confidence_product', 'weighted_sentiment',\n",
    "                                         'author_avg_sentiment', 'am_pm_sentiment_ratio',\n",
    "                                         'intraday_sentiment_volatility'])\n",
    "        return empty_df\n",
    "    \n",
    "    required_columns = ['Ticker', 'Date', 'Sentiment', 'Confidence', 'Prob_POS', \n",
    "                        'Prob_NTR', 'Prob_NEG', 'Relevance', 'SourceWeight', 'TopicWeight']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            if col in ['Ticker', 'Date', 'Sentiment']:\n",
    "                raise ValueError(f\"Required column {col} is missing in the data.\")\n",
    "            else:\n",
    "                df[col] = 0\n",
    "    \n",
    "    # Add Author column if missing\n",
    "    if 'Author' not in df.columns:\n",
    "        df['Author'] = 'unknown'\n",
    "    \n",
    "    # Create time-of-day features\n",
    "    df['hour'] = df['Received_Time_EST'].dt.hour\n",
    "    df['is_am'] = df['hour'] < 12\n",
    "    df['is_market_hours'] = (df['hour'] >= 9.5) & (df['hour'] <= 16)\n",
    "    \n",
    "    # Basic aggregations\n",
    "    agg_funcs = {\n",
    "        'Sentiment': ['mean', 'std', 'count', 'skew', 'median'],\n",
    "        'Confidence': ['mean', 'std'],\n",
    "        'Prob_POS': ['mean', 'max'],\n",
    "        'Prob_NTR': ['mean'],\n",
    "        'Prob_NEG': ['mean', 'max'],\n",
    "        'Relevance': ['mean', 'sum'],\n",
    "        'SourceWeight': ['mean'],\n",
    "        'TopicWeight': ['mean'],\n",
    "        'Author': ['nunique'],\n",
    "        'is_am': ['mean'],\n",
    "        'is_market_hours': ['mean']\n",
    "    }\n",
    "    \n",
    "    grouped = df.groupby(['Ticker', 'Date']).agg(agg_funcs)\n",
    "    grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "    grouped = grouped.reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    rename_dict = {\n",
    "        'Sentiment_mean': 'sentiment_mean',\n",
    "        'Sentiment_std': 'sentiment_std',\n",
    "        'Sentiment_count': 'post_count',\n",
    "        'Sentiment_skew': 'sentiment_skew',\n",
    "        'Sentiment_median': 'sentiment_median',\n",
    "        'Confidence_mean': 'avg_confidence',\n",
    "        'Confidence_std': 'confidence_std',\n",
    "        'Prob_POS_mean': 'avg_prob_pos',\n",
    "        'Prob_POS_max': 'max_prob_pos',\n",
    "        'Prob_NTR_mean': 'avg_prob_ntr',\n",
    "        'Prob_NEG_mean': 'avg_prob_neg',\n",
    "        'Prob_NEG_max': 'max_prob_neg',\n",
    "        'SourceWeight_mean': 'avg_source_weight',\n",
    "        'TopicWeight_mean': 'avg_topic_weight',\n",
    "        'Relevance_mean': 'avg_relevance',\n",
    "        'Relevance_sum': 'total_relevance',\n",
    "        'Author_nunique': 'unique_author_count',\n",
    "        'is_am_mean': 'morning_post_ratio',\n",
    "        'is_market_hours_mean': 'market_hours_ratio'\n",
    "    }\n",
    "    grouped = grouped.rename(columns=rename_dict)\n",
    "    \n",
    "    # Calculate additional derived features\n",
    "    \n",
    "    # Volume indicators\n",
    "    grouped['log_post_count'] = np.log1p(grouped['post_count'])\n",
    "    \n",
    "    # Advanced sentiment metrics\n",
    "    grouped['pos_neg_ratio'] = grouped['avg_prob_pos'] / (grouped['avg_prob_neg'] + 0.001)\n",
    "    grouped['sentiment_confidence_product'] = grouped['sentiment_mean'] * grouped['avg_confidence']\n",
    "    \n",
    "    # Calculate weighted sentiment metrics\n",
    "    def weighted_sentiment_func(sub_df):\n",
    "        if sub_df['Sentiment'].count() == 0:\n",
    "            return 0\n",
    "        return (sub_df['Sentiment'] * sub_df['Relevance']).sum() / sub_df['Relevance'].sum() if sub_df['Relevance'].sum() > 0 else 0\n",
    "    \n",
    "    def author_sentiment_avg(sub_df):\n",
    "        return sub_df.groupby('Author')['Sentiment'].mean().mean() if sub_df['Author'].nunique() > 0 else 0\n",
    "        \n",
    "    # Time-based metrics\n",
    "    def am_pm_sentiment_ratio(sub_df):\n",
    "        am_sentiment = sub_df[sub_df['is_am']]['Sentiment'].mean() if not sub_df[sub_df['is_am']].empty else 0\n",
    "        pm_sentiment = sub_df[~sub_df['is_am']]['Sentiment'].mean() if not sub_df[~sub_df['is_am']].empty else 0\n",
    "        return am_sentiment / (pm_sentiment + 0.001) if pm_sentiment != 0 else 1\n",
    "    \n",
    "    def intraday_volatility(sub_df):\n",
    "        return sub_df.groupby(['hour'])['Sentiment'].std().mean() if not sub_df.empty else 0\n",
    "    \n",
    "    # Special aggregations\n",
    "    special_aggs = df.groupby(['Ticker', 'Date']).apply(\n",
    "        lambda x: pd.Series({\n",
    "            'weighted_sentiment': weighted_sentiment_func(x),\n",
    "            'author_avg_sentiment': author_sentiment_avg(x),\n",
    "            'am_pm_sentiment_ratio': am_pm_sentiment_ratio(x),\n",
    "            'intraday_sentiment_volatility': intraday_volatility(x)\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Merge special aggregations\n",
    "    grouped = pd.merge(grouped, special_aggs, on=['Ticker', 'Date'], how='left')\n",
    "    \n",
    "    # Handle missing values\n",
    "    grouped = grouped.fillna(0)\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "####################################\n",
    "# PyTorch Neural Network Model\n",
    "####################################\n",
    "\n",
    "class FeedforwardNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64], dropout_rate=0.3):\n",
    "        \"\"\"\n",
    "        A feedforward neural network with configurable hidden layers and dropout.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Number of input features\n",
    "        hidden_dims : list\n",
    "            List of hidden layer dimensions\n",
    "        dropout_rate : float\n",
    "            Dropout probability for regularization\n",
    "        \"\"\"\n",
    "        super(FeedforwardNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "####################################\n",
    "# Main Functions\n",
    "####################################\n",
    "\n",
    "def train_model(sentiment_data, return_data):\n",
    "    \"\"\"\n",
    "    Train a model using sentiment features to predict next-day returns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentiment_data : DataFrame\n",
    "        The Reddit sentiment data for training.\n",
    "    return_data : DataFrame\n",
    "        The stock return data for training.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model_info : dict\n",
    "        Contains the trained PyTorch model, scaler, feature columns, and device information.\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing sentiment data...\")\n",
    "    sentiment_data = preprocess_sentiment_data(sentiment_data)\n",
    "    print(\"Creating features from sentiment data...\")\n",
    "    features_df = create_features(sentiment_data)\n",
    "    \n",
    "    # Preprocess return_data: ensure Date is datetime, Ticker uppercase, and convert Return.\n",
    "    return_data = return_data.copy()\n",
    "    return_data['Date'] = pd.to_datetime(return_data['Date'])\n",
    "    return_data['Ticker'] = return_data['Ticker'].str.upper()\n",
    "    return_data['Return'] = return_data['Return'].apply(convert_return)\n",
    "    \n",
    "    print(\"Merging sentiment features with stock returns...\")\n",
    "    model_data = pd.merge(features_df, return_data[['Date', 'Ticker', 'Return']], on=['Date', 'Ticker'], how='inner')\n",
    "    model_data = model_data.dropna(subset=['Return'])\n",
    "    \n",
    "    # Get all feature columns (excluding Date, Ticker, Return)\n",
    "    all_feature_columns = [col for col in model_data.columns if col not in ['Date', 'Ticker', 'Return']]\n",
    "    model_data[all_feature_columns] = model_data[all_feature_columns].fillna(0)\n",
    "    \n",
    "    # Sort data chronologically\n",
    "    model_data = model_data.sort_values('Date')\n",
    "    \n",
    "    # Use time series cross-validation for proper evaluation\n",
    "    unique_dates = np.sort(model_data['Date'].unique())\n",
    "    n_splits = 5\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    date_indices = np.arange(len(unique_dates))\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_scaler = None\n",
    "    \n",
    "    for train_idx, val_idx in tscv.split(date_indices):\n",
    "        train_dates = unique_dates[train_idx]\n",
    "        val_dates = unique_dates[val_idx]\n",
    "        \n",
    "        train_data = model_data[model_data['Date'].isin(train_dates)]\n",
    "        val_data = model_data[model_data['Date'].isin(val_dates)]\n",
    "        \n",
    "        X_train = train_data[all_feature_columns].values\n",
    "        y_train = train_data['Return'].values.reshape(-1, 1)\n",
    "        X_val = val_data[all_feature_columns].values\n",
    "        y_val = val_data['Return'].values.reshape(-1, 1)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "        \n",
    "        input_dim = X_train_tensor.shape[1]\n",
    "        \n",
    "        # Create model with improved architecture\n",
    "        model_net = FeedforwardNet(\n",
    "            input_dim=input_dim, \n",
    "            hidden_dims=[128, 64], \n",
    "            dropout_rate=0.3\n",
    "        ).to(device)\n",
    "        \n",
    "        # Define loss and optimizer with learning rate decay\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model_net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "        \n",
    "        # Early stopping setup\n",
    "        patience = 30\n",
    "        best_fold_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_fold_model_state = None\n",
    "        \n",
    "        # Training loop\n",
    "        print(f\"Training fold {len(train_dates)} days -> {len(val_dates)} days...\")\n",
    "        epochs = 300\n",
    "        for epoch in range(epochs):\n",
    "            model_net.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_net(X_train_tensor)\n",
    "            loss = criterion(outputs, y_train_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            model_net.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model_net(X_val_tensor)\n",
    "                val_loss = criterion(val_outputs, y_val_tensor)\n",
    "                scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss.item() < best_fold_loss:\n",
    "                best_fold_loss = val_loss.item()\n",
    "                patience_counter = 0\n",
    "                best_fold_model_state = model_net.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"Epoch {epoch:3d} | Train Loss: {loss.item():.6f} | Val Loss: {val_loss.item():.6f}\")\n",
    "        \n",
    "        # Restore best model from this fold\n",
    "        model_net.load_state_dict(best_fold_model_state)\n",
    "        \n",
    "        # Final validation evaluation\n",
    "        model_net.eval()\n",
    "        with torch.no_grad():\n",
    "            final_val_outputs = model_net(X_val_tensor)\n",
    "            final_val_loss = criterion(final_val_outputs, y_val_tensor)\n",
    "        \n",
    "        print(f\"Fold final validation loss: {final_val_loss.item():.6f}\")\n",
    "        \n",
    "        # Check if this is the best model across folds\n",
    "        if final_val_loss.item() < best_val_loss:\n",
    "            best_val_loss = final_val_loss.item()\n",
    "            best_model = model_net\n",
    "            best_scaler = scaler\n",
    "    \n",
    "    # Now train a final model on the complete dataset\n",
    "    print(\"Training final model on complete dataset...\")\n",
    "    X_all = model_data[all_feature_columns].values\n",
    "    y_all = model_data['Return'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Scale all data with the best scaler from cross-validation\n",
    "    X_all_scaled = best_scaler.transform(X_all)\n",
    "    X_all_tensor = torch.tensor(X_all_scaled, dtype=torch.float32).to(device)\n",
    "    y_all_tensor = torch.tensor(y_all, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Initialize the final model with the same architecture as best_model\n",
    "    final_model = FeedforwardNet(\n",
    "        input_dim=X_all_tensor.shape[1],\n",
    "        hidden_dims=[128, 64],\n",
    "        dropout_rate=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    # Copy weights from best model as starting point\n",
    "    final_model.load_state_dict(best_model.state_dict())\n",
    "    \n",
    "    # Define loss and optimizer for final training\n",
    "    final_optimizer = optim.Adam(final_model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "    final_criterion = nn.MSELoss()\n",
    "    \n",
    "    # Final training loop - fewer epochs to avoid overfitting\n",
    "    final_epochs = 100\n",
    "    for epoch in range(final_epochs):\n",
    "        final_model.train()\n",
    "        final_optimizer.zero_grad()\n",
    "        outputs = final_model(X_all_tensor)\n",
    "        loss = final_criterion(outputs, y_all_tensor)\n",
    "        loss.backward()\n",
    "        final_optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Final model - Epoch {epoch:3d} | Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    # Save feature importance (approximated by analyzing weights)\n",
    "    with torch.no_grad():\n",
    "        # Get weights from first layer\n",
    "        first_layer_weights = final_model.net[0].weight.cpu().numpy()\n",
    "        \n",
    "        # Calculate importance by taking the absolute value of weights\n",
    "        feature_importance = np.abs(first_layer_weights).mean(axis=0)\n",
    "        \n",
    "        # Create feature importance dictionary\n",
    "        importance_dict = dict(zip(all_feature_columns, feature_importance))\n",
    "        \n",
    "        # Sort features by importance\n",
    "        sorted_importance = dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "        \n",
    "        # Print top 10 features\n",
    "        print(\"\\nTop 10 most important features:\")\n",
    "        for i, (feat, imp) in enumerate(list(sorted_importance.items())[:10]):\n",
    "            print(f\"{i+1}. {feat}: {imp:.6f}\")\n",
    "    \n",
    "    model_info = {\n",
    "        'model': final_model,\n",
    "        'scaler': best_scaler,\n",
    "        'feature_columns': all_feature_columns,\n",
    "        'device': device,\n",
    "        'feature_importance': sorted_importance\n",
    "    }\n",
    "    \n",
    "    print(\"Training complete. Model is ready.\")\n",
    "    return model_info\n",
    "\n",
    "def predict_returns(model_info, sentiment_data_today, stock_universe_today, historical_data=None):\n",
    "    \"\"\"\n",
    "    Generate predictions of next-day returns for all stocks in the universe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_info : dict\n",
    "        Contains the trained model, scaler, and feature columns.\n",
    "    sentiment_data_today : DataFrame\n",
    "        New sentiment data (for a single day).\n",
    "    stock_universe_today : list\n",
    "        List of stock tickers available today.\n",
    "    historical_data : dict, optional\n",
    "        Not used in this implementation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predictions : DataFrame\n",
    "        A DataFrame with columns ['Ticker', 'Predicted_Return', 'Signal_Rank'].\n",
    "    \"\"\"\n",
    "    # Handle empty inputs\n",
    "    if sentiment_data_today.empty:\n",
    "        # Return default predictions for the stock universe\n",
    "        predictions = pd.DataFrame({\n",
    "            'Ticker': [ticker.upper() for ticker in stock_universe_today],\n",
    "            'Predicted_Return': [0.0] * len(stock_universe_today)\n",
    "        })\n",
    "        predictions['Signal_Rank'] = 0.5  # Neutral ranking for all stocks\n",
    "        return predictions\n",
    "    \n",
    "    # Preprocess the sentiment data\n",
    "    sentiment_data_today = preprocess_sentiment_data(sentiment_data_today)\n",
    "    \n",
    "    # Define current_date using the max date available in the preprocessed data\n",
    "    if sentiment_data_today.empty:\n",
    "        current_date = pd.Timestamp.today().normalize()\n",
    "    else:\n",
    "        current_date = sentiment_data_today['Date'].max()\n",
    "    \n",
    "    # Filter to today's sentiment data\n",
    "    sentiment_today = sentiment_data_today[sentiment_data_today['Date'] == current_date].copy()\n",
    "    \n",
    "    # Create features from sentiment data\n",
    "    features_today = create_features(sentiment_today) \n",
    "    \n",
    "    # Ensure all tickers in the universe are included\n",
    "    universe_upper = [t.upper() for t in stock_universe_today]\n",
    "    \n",
    "    # If features_today is empty or missing the 'Ticker' column, build a default features DataFrame\n",
    "    if features_today.empty or 'Ticker' not in features_today.columns:\n",
    "        default_data = []\n",
    "        for t in universe_upper:\n",
    "            default_row = {col: 0 for col in model_info['feature_columns']}\n",
    "            default_row['Ticker'] = t\n",
    "            default_row['Date'] = current_date\n",
    "            default_data.append(default_row)\n",
    "        features_today = pd.DataFrame(default_data)\n",
    "    else:\n",
    "        # Ensure the Ticker column is uppercase\n",
    "        features_today['Ticker'] = features_today['Ticker'].str.upper()\n",
    "        \n",
    "        # Filter features to include only tickers in stock_universe_today\n",
    "        features_today = features_today[features_today['Ticker'].isin(universe_upper)]\n",
    "        \n",
    "        # Add default rows for any tickers that are missing\n",
    "        existing_tickers = set(features_today['Ticker'])\n",
    "        missing_tickers = set(universe_upper) - existing_tickers\n",
    "        \n",
    "        if missing_tickers:\n",
    "            default_data = []\n",
    "            for t in missing_tickers:\n",
    "                default_row = {col: 0 for col in model_info['feature_columns']}\n",
    "                default_row['Ticker'] = t\n",
    "                default_row['Date'] = current_date\n",
    "                default_data.append(default_row)\n",
    "            \n",
    "            if default_data:\n",
    "                default_df = pd.DataFrame(default_data)\n",
    "                features_today = pd.concat([features_today, default_df], ignore_index=True)\n",
    "    \n",
    "    # Final safeguard: if features_today is still empty, force defaults for every ticker\n",
    "    if features_today.empty:\n",
    "        default_data = []\n",
    "        for t in universe_upper:\n",
    "            default_row = {col: 0 for col in model_info['feature_columns']}\n",
    "            default_row['Ticker'] = t\n",
    "            default_row['Date'] = current_date\n",
    "            default_data.append(default_row)\n",
    "        features_today = pd.DataFrame(default_data)\n",
    "    \n",
    "    # Make sure all required feature columns exist\n",
    "    for col in model_info['feature_columns']:\n",
    "        if col not in features_today.columns:\n",
    "            features_today[col] = 0\n",
    "    \n",
    "    # Sort values safely (only if Ticker exists)\n",
    "    if 'Ticker' in features_today.columns:\n",
    "        features_today = features_today.sort_values('Ticker').reset_index(drop=True)\n",
    "    \n",
    "    # Prepare features for prediction\n",
    "    X_today = features_today[model_info['feature_columns']].fillna(0).values\n",
    "    \n",
    "    if X_today.shape[0] == 0:\n",
    "        # If no data is present, create a default matrix\n",
    "        X_today = np.zeros((len(universe_upper), len(model_info['feature_columns'])))\n",
    "        features_today = pd.DataFrame({\n",
    "            'Ticker': universe_upper,\n",
    "        })\n",
    "        for col in model_info['feature_columns']:\n",
    "            features_today[col] = 0\n",
    "\n",
    "    # Scale features using the saved scaler\n",
    "    X_today_scaled = model_info['scaler'].transform(X_today)\n",
    "    X_today_tensor = torch.tensor(X_today_scaled, dtype=torch.float32).to(model_info['device'])\n",
    "    \n",
    "    # Generate predictions\n",
    "    model = model_info['model']\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_tensor = model(X_today_tensor)\n",
    "    predictions_array = predictions_tensor.cpu().numpy().flatten()\n",
    "    \n",
    "    # Add a small random noise to break any ties\n",
    "    predictions_array += np.random.normal(0, 1e-6, size=predictions_array.shape)\n",
    "    \n",
    "    # Create prediction DataFrame\n",
    "    predictions = pd.DataFrame({\n",
    "        'Ticker': features_today['Ticker'],\n",
    "        'Predicted_Return': predictions_array\n",
    "    })\n",
    "    \n",
    "    # Calculate percentile rank\n",
    "    predictions['Signal_Rank'] = predictions['Predicted_Return'].rank(pct=True)\n",
    "    \n",
    "    # Ensure all columns are included and in the right order\n",
    "    predictions = predictions[['Ticker', 'Predicted_Return', 'Signal_Rank']]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "####################################\n",
    "# Test Section (Runs if the script is executed directly)\n",
    "####################################\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        sentiment_data = pd.read_csv('./data/sentiment_train_2017_2021.csv')\n",
    "        return_data = pd.read_csv('./data/return_train_2017_2021.csv')\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Data files not found. Please check file paths.\")\n",
    "        sentiment_data = None\n",
    "        return_data = None\n",
    "\n",
    "    if sentiment_data is not None and return_data is not None:\n",
    "        model_info = train_model(sentiment_data, return_data)\n",
    "        \n",
    "        # For testing prediction, filter the sentiment data to a sample day\n",
    "        sample_day = pd.to_datetime('2021-06-01')\n",
    "        preprocessed_sentiment = preprocess_sentiment_data(sentiment_data)\n",
    "        sentiment_data_today = preprocessed_sentiment[preprocessed_sentiment['Date'] == sample_day].copy()\n",
    "        \n",
    "        stock_universe_today = return_data[return_data['Date'] == sample_day]['Ticker'].unique().tolist()\n",
    "        \n",
    "        predictions = predict_returns(model_info, sentiment_data_today, stock_universe_today)\n",
    "        print(\"Sample predictions:\")\n",
    "        print(predictions.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
