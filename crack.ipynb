{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acb406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "001_prediction_function.py\n",
    "\n",
    "This file provides two main functions:\n",
    "    1. train_model(sentiment_data, return_data)\n",
    "    2. predict_returns(model, sentiment_data_today, stock_universe_today)\n",
    "\n",
    "The code processes Alexandria’s Reddit sentiment data, creates aggregated features (including time‐series and topic‐based features),\n",
    "and trains a PyTorch feedforward neural network (with GPU support when available) to predict next-day stock returns.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime, time, timedelta\n",
    "\n",
    "# Set device for GPU usage if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "####################################\n",
    "# Helper Functions\n",
    "####################################\n",
    "\n",
    "def convert_return(x):\n",
    "    \"\"\"\n",
    "    Converts a return value to a float.\n",
    "    If x is a string ending in '%', removes the '%' and divides by 100.\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if x.endswith('%'):\n",
    "            return float(x[:-1].strip()) / 100.0\n",
    "        else:\n",
    "            return float(x)\n",
    "    else:\n",
    "        return float(x)\n",
    "\n",
    "def preprocess_sentiment_data(sentiment_data):\n",
    "    \"\"\"\n",
    "    Preprocess sentiment data:\n",
    "      - Convert 'Received_Time' to a timezone-aware datetime (UTC) then to US/Eastern.\n",
    "      - Create a 'Date' column by shifting posts received after 4:00 PM (EST) to the next day.\n",
    "      - Ensure 'Ticker' is uppercase.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentiment_data : DataFrame\n",
    "        Raw sentiment data containing at least 'Received_Time' and 'Ticker'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        Processed sentiment data with added 'Received_Time_EST' and 'Date' columns.\n",
    "    \"\"\"\n",
    "    df = sentiment_data.copy()\n",
    "    if 'Received_Time' not in df.columns:\n",
    "        raise ValueError(\"Column 'Received_Time' not found in the sentiment data.\")\n",
    "    df['Received_Time'] = pd.to_datetime(df['Received_Time'], utc=True)\n",
    "    df['Received_Time_EST'] = df['Received_Time'].dt.tz_convert('America/New_York')\n",
    "    df['local_date'] = df['Received_Time_EST'].dt.date\n",
    "    cutoff = time(16, 0)  # 4:00 PM cutoff.\n",
    "    df['Date'] = df['Received_Time_EST'].apply(\n",
    "        lambda x: pd.to_datetime(x.date() + timedelta(days=1)) if x.time() > cutoff else pd.to_datetime(x.date())\n",
    "    )\n",
    "    if 'Ticker' in df.columns:\n",
    "        df['Ticker'] = df['Ticker'].str.upper()\n",
    "    return df\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create advanced daily features by aggregating sentiment data for each Ticker and Date,\n",
    "    and compute additional time-series and topic-based features.\n",
    "    \n",
    "    Base aggregated features (per ticker and date):\n",
    "      - sentiment_mean, sentiment_std, post_count, avg_confidence, avg_prob_pos, avg_prob_ntr,\n",
    "        avg_prob_neg, avg_source_weight, avg_topic_weight, avg_relevance, weighted_sentiment,\n",
    "        net_sentiment (sum of Sentiment).\n",
    "    \n",
    "    Additional time-series features (computed for each ticker):\n",
    "      - cumulative_sentiment: cumulative sum of net_sentiment.\n",
    "      - daily_sentiment_change: difference of net_sentiment from the previous day.\n",
    "      - ma_5: 5-day moving average of net_sentiment.\n",
    "      - ma_10: 10-day moving average of net_sentiment.\n",
    "      - past_3_sentiment: sum of net_sentiment for the previous 3 days (with a one-day lag).\n",
    "      - log_volume: log(1 + post_count).\n",
    "    \n",
    "    Topic-based features:\n",
    "      - For each target topic (from the provided list), create two features:\n",
    "          - WSB_count_<topic>: count of posts for that topic from sources that contain \"WSB\".\n",
    "          - INVESTING_count_<topic>: count of posts for that topic from sources that contain \"INVESTING\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Preprocessed sentiment data with required columns. Optionally, it should have:\n",
    "        'Reddit_Topic' and 'Source'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    final_df : DataFrame\n",
    "        Aggregated features for each Ticker and Date including additional time-series and topic-based features.\n",
    "    \"\"\"\n",
    "    # Ensure required columns exist.\n",
    "    required_columns = ['Ticker', 'Date', 'Sentiment', 'Confidence', 'Prob_POS', \n",
    "                        'Prob_NTR', 'Prob_NEG', 'Relevance', 'SourceWeight', 'TopicWeight']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            if col in ['Ticker', 'Date', 'Sentiment']:\n",
    "                raise ValueError(f\"Required column {col} is missing in the data.\")\n",
    "            else:\n",
    "                df[col] = 0\n",
    "\n",
    "    # Base aggregation: calculate mean, std, count, and sum (for net sentiment).\n",
    "    agg_funcs = {\n",
    "        'Sentiment': ['mean', 'std', 'count', 'sum'],\n",
    "        'Confidence': 'mean',\n",
    "        'Prob_POS': 'mean',\n",
    "        'Prob_NTR': 'mean',\n",
    "        'Prob_NEG': 'mean',\n",
    "        'Relevance': 'mean',\n",
    "        'SourceWeight': 'mean',\n",
    "        'TopicWeight': 'mean'\n",
    "    }\n",
    "    grouped = df.groupby(['Ticker', 'Date']).agg(agg_funcs)\n",
    "    grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "    grouped = grouped.reset_index()\n",
    "    rename_dict = {\n",
    "        'Sentiment_mean': 'sentiment_mean',\n",
    "        'Sentiment_std': 'sentiment_std',\n",
    "        'Sentiment_count': 'post_count',\n",
    "        'Sentiment_sum': 'net_sentiment',\n",
    "        'Confidence_mean': 'avg_confidence',\n",
    "        'Prob_POS_mean': 'avg_prob_pos',\n",
    "        'Prob_NTR_mean': 'avg_prob_ntr',\n",
    "        'Prob_NEG_mean': 'avg_prob_neg',\n",
    "        'SourceWeight_mean': 'avg_source_weight',\n",
    "        'TopicWeight_mean': 'avg_topic_weight',\n",
    "        'Relevance_mean': 'avg_relevance'\n",
    "    }\n",
    "    grouped = grouped.rename(columns=rename_dict)\n",
    "    \n",
    "    # Compute weighted sentiment: average (Sentiment * Relevance)\n",
    "    def weighted_sentiment_func(sub_df):\n",
    "        if sub_df['Sentiment'].count() == 0:\n",
    "            return 0\n",
    "        return (sub_df['Sentiment'] * sub_df['Relevance']).sum() / sub_df['Sentiment'].count()\n",
    "    ws = df.groupby(['Ticker', 'Date']).apply(weighted_sentiment_func).reset_index(name='weighted_sentiment')\n",
    "    grouped = pd.merge(grouped, ws, on=['Ticker', 'Date'], how='left')\n",
    "    \n",
    "    # Compute time-series features per ticker.\n",
    "    def compute_time_series_features(sub_df):\n",
    "        sub_df = sub_df.sort_values('Date').copy()\n",
    "        sub_df['cumulative_sentiment'] = sub_df['net_sentiment'].cumsum()\n",
    "        sub_df['daily_sentiment_change'] = sub_df['net_sentiment'].diff().fillna(0)\n",
    "        sub_df['ma_5'] = sub_df['net_sentiment'].rolling(window=5, min_periods=1).mean()\n",
    "        sub_df['ma_10'] = sub_df['net_sentiment'].rolling(window=10, min_periods=1).mean()\n",
    "        sub_df['past_3_sentiment'] = sub_df['net_sentiment'].shift(1).rolling(window=3, min_periods=1).sum().fillna(0)\n",
    "        sub_df['log_volume'] = np.log1p(sub_df['post_count'])\n",
    "        return sub_df\n",
    "    ts_features = grouped.groupby('Ticker').apply(compute_time_series_features).reset_index(drop=True)\n",
    "    \n",
    "    # Now add topic-based features if 'Reddit_Topic' and 'Source' exist.\n",
    "    all_topics = [\n",
    "        \"Biotech\", \"Chart\", \"Commentary\", \"Daily Discussion\", \"Daily Thread\", \"DD\", \"Discussion\", \"Distressed\",\n",
    "        \"Earnings Thread\", \"Education\", \"Energy\", \"Fundamentals\", \"Futures\", \"Gain\", \"Help\", \"Industry Report\",\n",
    "        \"Interview/Profile\", \"Investor Letter\", \"Long Thesis\", \"Loss\", \"Macro\", \"Meme\", \"Mods\", \"News\", \"None\",\n",
    "        \"Options\", \"Profit\", \"Question\", \"Retail\", \"Satire\", \"Shitpost\", \"Short Thesis\", \"Special Situation\",\n",
    "        \"Stocks\", \"Storytime\", \"Strategy\", \"tag me pls\", \"Technicals\", \"Thesis\", \"Wall St. \\\"Leaks\\\"\",\n",
    "        \"Weekend Discussion\", \"WSBbooks\", \"YOLO\"\n",
    "    ]\n",
    "    \n",
    "    if 'Reddit_Topic' in df.columns and 'Source' in df.columns:\n",
    "        # Create a copy for topic computations.\n",
    "        df_topic = df.copy()\n",
    "        df_topic['Reddit_Topic'] = df_topic['Reddit_Topic'].fillna(\"\").str.upper()\n",
    "        df_topic['Source'] = df_topic['Source'].fillna(\"\").str.upper()\n",
    "        \n",
    "        # Define two source categories: \"WSB\" and \"INVESTING\".\n",
    "        for src in [\"WSB\", \"INVESTING\"]:\n",
    "            src_mask = df_topic['Source'].str.contains(src, case=False, na=False)\n",
    "            df_src = df_topic[src_mask]\n",
    "            # Group by Ticker, Date and Reddit_Topic, then count posts.\n",
    "            topic_counts = df_src.groupby(['Ticker', 'Date'])['Reddit_Topic'].value_counts().unstack(fill_value=0)\n",
    "            # For each topic in our list, ensure there is a column.\n",
    "            for topic in all_topics:\n",
    "                topic_upper = topic.upper()\n",
    "                col_name = f\"{src}_count_{topic_upper}\"\n",
    "                if topic_upper in topic_counts.columns:\n",
    "                    # Rename existing column to the standardized name.\n",
    "                    topic_counts = topic_counts.rename(columns={topic_upper: col_name})\n",
    "                else:\n",
    "                    # Create the column with default value 0.\n",
    "                    topic_counts[col_name] = 0\n",
    "            # Reset index so that Ticker and Date become columns.\n",
    "            topic_counts = topic_counts.reset_index()[['Ticker', 'Date'] + [f\"{src}_count_{t.upper()}\" for t in all_topics]]\n",
    "            # Merge with the ts_features DataFrame.\n",
    "            ts_features = pd.merge(ts_features, topic_counts, on=['Ticker', 'Date'], how='left')\n",
    "            # Fill missing topic count columns with 0.\n",
    "            for topic in all_topics:\n",
    "                col_name = f\"{src}_count_{topic.upper()}\"\n",
    "                if col_name in ts_features.columns:\n",
    "                    ts_features[col_name] = ts_features[col_name].fillna(0)\n",
    "    \n",
    "    final_df = ts_features.fillna(0)\n",
    "    return final_df\n",
    "\n",
    "####################################\n",
    "# Updated Neural Network Model with Hyperparameter Optimizations\n",
    "####################################\n",
    "\n",
    "class FeedforwardNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        A feedforward neural network with two hidden layers, dropout, and updated architecture.\n",
    "        \"\"\"\n",
    "        super(FeedforwardNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "####################################\n",
    "# Main Functions\n",
    "####################################\n",
    "\n",
    "def train_model(sentiment_data, return_data):\n",
    "    \"\"\"\n",
    "    Train a model using sentiment features to predict next-day returns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentiment_data : DataFrame\n",
    "        The Reddit sentiment data for training.\n",
    "    return_data : DataFrame\n",
    "        The stock return data for training.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model_info : dict\n",
    "        Contains the trained PyTorch model, scaler, feature columns, and device information.\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing sentiment data...\")\n",
    "    sentiment_data = preprocess_sentiment_data(sentiment_data)\n",
    "    print(\"Creating features from sentiment data...\")\n",
    "    features_df = create_features(sentiment_data)\n",
    "    \n",
    "    # Preprocess return_data: convert Date to datetime and normalize, ensure Ticker is uppercase, and convert Return.\n",
    "    return_data = return_data.copy()\n",
    "    return_data['Date'] = pd.to_datetime(return_data['Date']).dt.normalize()\n",
    "    return_data['Ticker'] = return_data['Ticker'].str.upper()\n",
    "    return_data['Return'] = return_data['Return'].apply(convert_return)\n",
    "    \n",
    "    print(\"Merging sentiment features with stock returns...\")\n",
    "    model_data = pd.merge(features_df, return_data[['Date', 'Ticker', 'Return']], on=['Date', 'Ticker'], how='inner')\n",
    "    model_data = model_data.dropna(subset=['Return'])\n",
    "    model_data['Return'] = model_data['Return'].apply(convert_return).astype(float)\n",
    "    \n",
    "    # Update feature_columns to include the additional features.\n",
    "    feature_columns = [\n",
    "        'sentiment_mean', 'sentiment_std', 'post_count', 'avg_confidence',\n",
    "        'avg_prob_pos', 'avg_prob_ntr', 'avg_prob_neg', 'weighted_sentiment',\n",
    "        'avg_source_weight', 'avg_topic_weight', 'net_sentiment',\n",
    "        'cumulative_sentiment', 'daily_sentiment_change', 'ma_5', 'ma_10',\n",
    "        'past_3_sentiment', 'log_volume'\n",
    "    ]\n",
    "    # Also add topic count features for both sources.\n",
    "    for src in [\"WSB\", \"INVESTING\"]:\n",
    "        for topic in [t.upper() for t in [\n",
    "            \"Biotech\", \"Chart\", \"Commentary\", \"Daily Discussion\", \"Daily Thread\", \"DD\", \"Discussion\", \"Distressed\",\n",
    "            \"Earnings Thread\", \"Education\", \"Energy\", \"Fundamentals\", \"Futures\", \"Gain\", \"Help\", \"Industry Report\",\n",
    "            \"Interview/Profile\", \"Investor Letter\", \"Long Thesis\", \"Loss\", \"Macro\", \"Meme\", \"Mods\", \"News\", \"None\",\n",
    "            \"Options\", \"Profit\", \"Question\", \"Retail\", \"Satire\", \"Shitpost\", \"Short Thesis\", \"Special Situation\",\n",
    "            \"Stocks\", \"Storytime\", \"Strategy\", \"tag me pls\", \"Technicals\", \"Thesis\", \"Wall St. \\\"Leaks\\\"\",\n",
    "            \"Weekend Discussion\", \"WSBbooks\", \"YOLO\"\n",
    "        ]]:\n",
    "            feature_columns.append(f\"{src}_count_{topic}\")\n",
    "    \n",
    "    model_data[feature_columns] = model_data[feature_columns].fillna(0)\n",
    "    model_data = model_data.sort_values('Date')\n",
    "    unique_dates = np.sort(model_data['Date'].unique())\n",
    "    split_index = int(0.8 * len(unique_dates))\n",
    "    train_dates = unique_dates[:split_index]\n",
    "    val_dates = unique_dates[split_index:]\n",
    "    \n",
    "    train_data = model_data[model_data['Date'].isin(train_dates)]\n",
    "    val_data = model_data[model_data['Date'].isin(val_dates)]\n",
    "    \n",
    "    X_train = train_data[feature_columns].values\n",
    "    y_train = train_data['Return'].values.reshape(-1, 1)\n",
    "    X_val = val_data[feature_columns].values\n",
    "    y_val = val_data['Return'].values.reshape(-1, 1)\n",
    "    \n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "    \n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "    model_net = FeedforwardNet(input_dim).to(device)\n",
    "    \n",
    "    # Use AdamW optimizer with updated learning rate and weight decay.\n",
    "    optimizer = optim.AdamW(model_net.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    print(\"Training the model with updated hyperparameters...\")\n",
    "    epochs = 250\n",
    "    for epoch in range(epochs):\n",
    "        model_net.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_net(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 25 == 0:\n",
    "            model_net.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model_net(X_val_tensor)\n",
    "                val_loss = criterion(val_outputs, y_val_tensor)\n",
    "            print(f\"Epoch {epoch:3d} | Train Loss: {loss.item():.6f} | Val Loss: {val_loss.item():.6f}\")\n",
    "    \n",
    "    model_info = {\n",
    "        'model': model_net,\n",
    "        'scaler': scaler,\n",
    "        'feature_columns': feature_columns,\n",
    "        'device': device\n",
    "    }\n",
    "    print(\"Training complete. Model is ready.\")\n",
    "    return model_info\n",
    "\n",
    "def predict_returns(model, sentiment_data_today, stock_universe_today):\n",
    "    \"\"\"\n",
    "    Generate predictions of next-day returns for all stocks in the universe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : dict\n",
    "        Contains the trained model, scaler, feature columns, and device info.\n",
    "    sentiment_data_today : DataFrame\n",
    "        New sentiment data (for a single day).\n",
    "    stock_universe_today : list\n",
    "        List of stock tickers available today.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predictions : DataFrame\n",
    "        A DataFrame with columns ['Ticker', 'Predicted_Return', 'Signal_Rank'].\n",
    "    \"\"\"\n",
    "    # Preprocess today's sentiment data.\n",
    "    sentiment_data_today = preprocess_sentiment_data(sentiment_data_today)\n",
    "    if sentiment_data_today.empty:\n",
    "        current_date = pd.Timestamp.today().normalize()\n",
    "    else:\n",
    "        current_date = sentiment_data_today['Date'].max()\n",
    "    sentiment_today = sentiment_data_today[sentiment_data_today['Date'] == current_date].copy()\n",
    "    features_today = create_features(sentiment_today)\n",
    "    universe_upper = [t.upper() for t in stock_universe_today]\n",
    "    if features_today.empty or 'Ticker' not in features_today.columns:\n",
    "        default_data = []\n",
    "        for t in universe_upper:\n",
    "            default_row = {col: 0 for col in model['feature_columns']}\n",
    "            default_row['Ticker'] = t\n",
    "            default_row['Date'] = current_date\n",
    "            default_data.append(default_row)\n",
    "        features_today = pd.DataFrame(default_data)\n",
    "    else:\n",
    "        features_today['Ticker'] = features_today['Ticker'].str.upper()\n",
    "        features_today = features_today[features_today['Ticker'].isin(universe_upper)]\n",
    "        existing_tickers = set(features_today['Ticker'])\n",
    "        missing_tickers = set(universe_upper) - existing_tickers\n",
    "        if missing_tickers:\n",
    "            default_data = []\n",
    "            for t in missing_tickers:\n",
    "                default_row = {col: 0 for col in model['feature_columns']}\n",
    "                default_row['Ticker'] = t\n",
    "                default_row['Date'] = current_date\n",
    "                default_data.append(default_row)\n",
    "            if default_data:\n",
    "                default_df = pd.DataFrame(default_data)\n",
    "                features_today = pd.concat([features_today, default_df], ignore_index=True)\n",
    "    if features_today.empty:\n",
    "        default_data = []\n",
    "        for t in universe_upper:\n",
    "            default_row = {col: 0 for col in model['feature_columns']}\n",
    "            default_row['Ticker'] = t\n",
    "            default_row['Date'] = current_date\n",
    "            default_data.append(default_row)\n",
    "        features_today = pd.DataFrame(default_data)\n",
    "    for col in model['feature_columns']:\n",
    "        if col not in features_today.columns:\n",
    "            features_today[col] = 0\n",
    "    if 'Ticker' not in features_today.columns:\n",
    "        features_today['Ticker'] = \"\"\n",
    "    features_today = features_today.sort_values('Ticker').reset_index(drop=True)\n",
    "    X_today = features_today[model['feature_columns']].fillna(0).values\n",
    "    if X_today.shape[0] == 0:\n",
    "        X_today = np.zeros((len(universe_upper), len(model['feature_columns'])))\n",
    "        features_today = pd.DataFrame({'Ticker': universe_upper})\n",
    "        for col in model['feature_columns']:\n",
    "            features_today[col] = 0\n",
    "    X_today_scaled = model['scaler'].transform(X_today)\n",
    "    X_today_tensor = torch.tensor(X_today_scaled, dtype=torch.float32).to(model['device'])\n",
    "    model_obj = model['model']\n",
    "    model_obj.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_tensor = model_obj(X_today_tensor)\n",
    "    predictions_array = predictions_tensor.cpu().numpy().flatten()\n",
    "    predictions_array += np.random.normal(0, 1e-6, size=predictions_array.shape)\n",
    "    features_today['Predicted_Return'] = predictions_array\n",
    "    features_today['Signal_Rank'] = features_today['Predicted_Return'].rank(pct=True)\n",
    "    predictions = features_today[['Ticker', 'Predicted_Return', 'Signal_Rank']].copy()\n",
    "    return predictions\n",
    "\n",
    "####################################\n",
    "# Test Section (Runs if the script is executed directly)\n",
    "####################################\n",
    "if __name__ == \"__main__\":\n",
    "    sentiment_data = pd.read_csv('./data/sentiment_train_2017_2021.csv')\n",
    "    return_data = pd.read_csv('./data/return_train_2017_2021.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "    return_data['Date'] = pd.to_datetime(return_data['Date']).dt.normalize()\n",
    "    model_info = train_model(sentiment_data, return_data)\n",
    "    sample_day = pd.to_datetime('2021-06-01').normalize()\n",
    "    preprocessed_sentiment = preprocess_sentiment_data(sentiment_data)\n",
    "    sentiment_data_today = preprocessed_sentiment[preprocessed_sentiment['Date'] == sample_day].copy()\n",
    "    stock_universe_today = return_data[return_data['Date'] == sample_day]['Ticker'].unique().tolist()\n",
    "    predictions = predict_returns(model=model_info, sentiment_data_today=sentiment_data_today, stock_universe_today=stock_universe_today)\n",
    "    print(\"Sample predictions:\")\n",
    "    print(predictions.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
