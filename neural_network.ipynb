{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27b0a8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]\n",
      "PyTorch built with CUDA: 12.8\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import torch\n",
    "print(\"PyTorch built with CUDA:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9625e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "001_prediction_function.py\n",
    "\n",
    "This file provides two main functions:\n",
    "    1. train_model(sentiment_data, return_data)\n",
    "    2. predict_returns(model_info, sentiment_data_today, stock_universe_today, historical_data=None)\n",
    "\n",
    "The code uses advanced feature engineering with Alexandria’s Reddit equity data (leveraging fields such as Relevance, Confidence, Probabilities, SourceWeight, TopicWeight, etc.),\n",
    "and trains a feedforward neural network using PyTorch on GPU (if available) to predict next-day stock returns.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime, time, timedelta\n",
    "\n",
    "# Set device for GPU usage if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f541af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Helper Functions\n",
    "####################################\n",
    "\n",
    "def convert_return(x):\n",
    "    \"\"\"\n",
    "    Converts a return value to a float.\n",
    "    If x is a string ending in '%', removes the '%' and divides by 100.\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if x.endswith('%'):\n",
    "            return float(x[:-1].strip()) / 100.0\n",
    "        else:\n",
    "            return float(x)\n",
    "    else:\n",
    "        return float(x)\n",
    "\n",
    "def preprocess_sentiment_data(sentiment_data):\n",
    "    \"\"\"\n",
    "    Preprocess sentiment data:\n",
    "      - Convert 'Received_Time' to a timezone-aware datetime (UTC) then to US/Eastern.\n",
    "      - Create a 'Date' column by shifting posts received after 4:00 PM (EST) to the next day.\n",
    "      - Ensure 'Ticker' is uppercase.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentiment_data : DataFrame\n",
    "        Raw sentiment data containing at least 'Received_Time' and 'Ticker'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        Processed sentiment data with added 'Received_Time_EST' and 'Date' columns.\n",
    "    \"\"\"\n",
    "    df = sentiment_data.copy()\n",
    "    if 'Received_Time' not in df.columns:\n",
    "        raise ValueError(\"Column 'Received_Time' not found in the sentiment data.\")\n",
    "    \n",
    "    # Convert 'Received_Time' to UTC datetime and then to US/Eastern.\n",
    "    df['Received_Time'] = pd.to_datetime(df['Received_Time'], utc=True)\n",
    "    df['Received_Time_EST'] = df['Received_Time'].dt.tz_convert('America/New_York')\n",
    "    \n",
    "    # Create local_date from the EST time.\n",
    "    df['local_date'] = df['Received_Time_EST'].dt.date\n",
    "    cutoff = time(16, 0)  # 4:00 PM cutoff\n",
    "    \n",
    "    # Shift to next day if time is after cutoff.\n",
    "    df['Date'] = df['Received_Time_EST'].apply(\n",
    "        lambda x: pd.to_datetime(x.date() + timedelta(days=1)) if x.time() > cutoff else pd.to_datetime(x.date())\n",
    "    )\n",
    "    \n",
    "    if 'Ticker' in df.columns:\n",
    "        df['Ticker'] = df['Ticker'].str.upper()\n",
    "    return df\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create advanced daily features by aggregating sentiment data for each Ticker and Date.\n",
    "    \n",
    "    Features include:\n",
    "      - sentiment_mean: Average sentiment.\n",
    "      - sentiment_std: Standard deviation of sentiment.\n",
    "      - post_count: Count of posts.\n",
    "      - avg_confidence: Average confidence.\n",
    "      - avg_prob_pos, avg_prob_ntr, avg_prob_neg: Averages of probabilities.\n",
    "      - avg_source_weight, avg_topic_weight: Averages from respective columns.\n",
    "      - weighted_sentiment: Average (Sentiment * Relevance) per post.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Preprocessed sentiment data with columns including \n",
    "        ['Ticker', 'Date', 'Sentiment', 'Confidence', 'Prob_POS', 'Prob_NTR', 'Prob_NEG',\n",
    "         'Relevance', 'SourceWeight', 'TopicWeight'].\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grouped : DataFrame\n",
    "        Aggregated features for each Ticker and Date.\n",
    "    \"\"\"\n",
    "    # Ensure required columns exist; create missing optional ones as 0.\n",
    "    required_columns = ['Ticker', 'Date', 'Sentiment', 'Confidence', 'Prob_POS', \n",
    "                        'Prob_NTR', 'Prob_NEG', 'Relevance', 'SourceWeight', 'TopicWeight']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            if col in ['Ticker', 'Date', 'Sentiment']:\n",
    "                raise ValueError(f\"Required column {col} is missing in the data.\")\n",
    "            else:\n",
    "                df[col] = 0\n",
    "\n",
    "    # Group by Ticker and Date and aggregate.\n",
    "    agg_funcs = {\n",
    "        'Sentiment': ['mean', 'std', 'count'],\n",
    "        'Confidence': 'mean',\n",
    "        'Prob_POS': 'mean',\n",
    "        'Prob_NTR': 'mean',\n",
    "        'Prob_NEG': 'mean',\n",
    "        'Relevance': 'mean',\n",
    "        'SourceWeight': 'mean',\n",
    "        'TopicWeight': 'mean'\n",
    "    }\n",
    "    grouped = df.groupby(['Ticker', 'Date']).agg(agg_funcs)\n",
    "    grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "    grouped = grouped.reset_index()\n",
    "    \n",
    "    grouped = grouped.rename(columns={\n",
    "        'Sentiment_mean': 'sentiment_mean',\n",
    "        'Sentiment_std': 'sentiment_std',\n",
    "        'Sentiment_count': 'post_count',\n",
    "        'Confidence_mean': 'avg_confidence',\n",
    "        'Prob_POS_mean': 'avg_prob_pos',\n",
    "        'Prob_NTR_mean': 'avg_prob_ntr',\n",
    "        'Prob_NEG_mean': 'avg_prob_neg',\n",
    "        'SourceWeight_mean': 'avg_source_weight',\n",
    "        'TopicWeight_mean': 'avg_topic_weight',\n",
    "        'Relevance_mean': 'avg_relevance'\n",
    "    })\n",
    "    \n",
    "    # Compute weighted_sentiment: average of (Sentiment * Relevance)\n",
    "    def weighted_sentiment_func(sub_df):\n",
    "        if sub_df['Sentiment'].count() == 0:\n",
    "            return 0\n",
    "        return (sub_df['Sentiment'] * sub_df['Relevance']).sum() / sub_df['Sentiment'].count()\n",
    "    \n",
    "    ws = df.groupby(['Ticker', 'Date']).apply(weighted_sentiment_func).reset_index(name='weighted_sentiment')\n",
    "    grouped = pd.merge(grouped, ws, on=['Ticker', 'Date'], how='left')\n",
    "    return grouped\n",
    "\n",
    "####################################\n",
    "# PyTorch Neural Network Model\n",
    "####################################\n",
    "\n",
    "class FeedforwardNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        A simple feedforward neural network with two hidden layers.\n",
    "        \"\"\"\n",
    "        super(FeedforwardNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "####################################\n",
    "# Main Functions\n",
    "####################################\n",
    "\n",
    "def train_model(sentiment_data, return_data):\n",
    "    \"\"\"\n",
    "    Train a model using sentiment features to predict next-day returns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentiment_data : DataFrame\n",
    "        The Reddit sentiment data for training.\n",
    "    return_data : DataFrame\n",
    "        The stock return data for training.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model_info : dict\n",
    "        Contains the trained PyTorch model, scaler, feature columns, and device information.\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing sentiment data...\")\n",
    "    sentiment_data = preprocess_sentiment_data(sentiment_data)\n",
    "    print(\"Creating features from sentiment data...\")\n",
    "    features_df = create_features(sentiment_data)\n",
    "    \n",
    "    # Preprocess return_data: ensure Date is datetime, Ticker uppercase, and convert Return.\n",
    "    return_data = return_data.copy()\n",
    "    return_data['Date'] = pd.to_datetime(return_data['Date'])\n",
    "    return_data['Ticker'] = return_data['Ticker'].str.upper()\n",
    "    return_data['Return'] = return_data['Return'].apply(convert_return)\n",
    "    \n",
    "    print(\"Merging sentiment features with stock returns...\")\n",
    "    model_data = pd.merge(features_df, return_data[['Date', 'Ticker', 'Return']], on=['Date', 'Ticker'], how='inner')\n",
    "    model_data = model_data.dropna(subset=['Return'])\n",
    "    \n",
    "    # Re-apply conversion to ensure numeric Return and fill missing values.\n",
    "    model_data['Return'] = model_data['Return'].apply(convert_return).astype(float)\n",
    "    feature_columns = ['sentiment_mean', 'sentiment_std', 'post_count', 'avg_confidence', \n",
    "                         'avg_prob_pos', 'avg_prob_ntr', 'avg_prob_neg', 'weighted_sentiment', \n",
    "                         'avg_source_weight', 'avg_topic_weight']\n",
    "    model_data[feature_columns] = model_data[feature_columns].fillna(0)\n",
    "    \n",
    "    # Sort data by Date to perform a time-based split.\n",
    "    model_data = model_data.sort_values('Date')\n",
    "    unique_dates = np.sort(model_data['Date'].unique())\n",
    "    split_index = int(0.8 * len(unique_dates))\n",
    "    train_dates = unique_dates[:split_index]\n",
    "    val_dates = unique_dates[split_index:]\n",
    "    \n",
    "    train_data = model_data[model_data['Date'].isin(train_dates)]\n",
    "    val_data = model_data[model_data['Date'].isin(val_dates)]\n",
    "    \n",
    "    X_train = train_data[feature_columns].values\n",
    "    y_train = train_data['Return'].values.reshape(-1, 1)\n",
    "    X_val = val_data[feature_columns].values\n",
    "    y_val = val_data['Return'].values.reshape(-1, 1)\n",
    "    \n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Convert numpy arrays to torch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "    \n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "    model_net = FeedforwardNet(input_dim).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model_net.parameters(), lr=0.001)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    epochs = 2000\n",
    "    for epoch in range(epochs):\n",
    "        model_net.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_net(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            model_net.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model_net(X_val_tensor)\n",
    "                val_loss = criterion(val_outputs, y_val_tensor)\n",
    "            # A warning about converting tensors with requires_grad can be ignored in this print statement.\n",
    "            print(f\"Epoch {epoch:3d} | Train Loss: {loss.item():.6f} | Val Loss: {val_loss.item():.6f}\")\n",
    "    \n",
    "    model_info = {\n",
    "        'model': model_net,\n",
    "        'scaler': scaler,\n",
    "        'feature_columns': feature_columns,\n",
    "        'device': device\n",
    "    }\n",
    "    print(\"Training complete. Model is ready.\")\n",
    "    return model_info\n",
    "\n",
    "def predict_returns(model_info, sentiment_data_today, stock_universe_today, historical_data=None):\n",
    "    \"\"\"\n",
    "    Generate predictions of next-day returns for all stocks in the universe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_info : dict\n",
    "        Contains the trained model, scaler, and feature columns.\n",
    "    sentiment_data_today : DataFrame\n",
    "        New sentiment data (for a single day).\n",
    "    stock_universe_today : list\n",
    "        List of stock tickers available today.\n",
    "    historical_data : dict, optional\n",
    "        Not used in this implementation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predictions : DataFrame\n",
    "        A DataFrame with columns ['Ticker', 'Predicted_Return', 'Signal_Rank'].\n",
    "    \"\"\"\n",
    "    # Preprocess today's sentiment data\n",
    "    sentiment_data_today = preprocess_sentiment_data(sentiment_data_today)\n",
    "    \n",
    "    # Ensure we have the 'Date' column available by preprocessing the raw data.\n",
    "    if sentiment_data_today.empty:\n",
    "        current_date = pd.Timestamp.today().normalize()\n",
    "        features_today = pd.DataFrame(columns=['Ticker', 'Date'] + model_info['feature_columns'])\n",
    "    else:\n",
    "        current_date = sentiment_data_today['Date'].max()\n",
    "        # Filter to only today's data.\n",
    "        sentiment_today = sentiment_data_today[sentiment_data_today['Date'] == current_date].copy()\n",
    "        features_today = create_features(sentiment_today)\n",
    "    \n",
    "    # If features_today is empty or lacks 'Ticker', create default rows.\n",
    "    if features_today.empty or 'Ticker' not in features_today.columns:\n",
    "        default_data = []\n",
    "        universe_upper = [t.upper() for t in stock_universe_today]\n",
    "        for t in universe_upper:\n",
    "            default_row = {col: 0 for col in model_info['feature_columns']}\n",
    "            default_row['Ticker'] = t\n",
    "            default_row['Date'] = current_date\n",
    "            default_data.append(default_row)\n",
    "        features_today = pd.DataFrame(default_data)\n",
    "    else:\n",
    "        # Ensure 'Ticker' is uppercase.\n",
    "        features_today['Ticker'] = features_today['Ticker'].str.upper()\n",
    "        universe_upper = [t.upper() for t in stock_universe_today]\n",
    "        features_today = features_today[features_today['Ticker'].isin(universe_upper)]\n",
    "        # Add default rows for any missing tickers.\n",
    "        existing_tickers = set(features_today['Ticker'])\n",
    "        missing_tickers = set(universe_upper) - existing_tickers\n",
    "        if missing_tickers:\n",
    "            default_data = []\n",
    "            for t in missing_tickers:\n",
    "                default_row = {col: 0 for col in model_info['feature_columns']}\n",
    "                default_row['Ticker'] = t\n",
    "                default_row['Date'] = current_date\n",
    "                default_data.append(default_row)\n",
    "            if default_data:\n",
    "                default_df = pd.DataFrame(default_data)\n",
    "                features_today = pd.concat([features_today, default_df], ignore_index=True)\n",
    "    \n",
    "    features_today = features_today.sort_values('Ticker').reset_index(drop=True)\n",
    "    \n",
    "    # Prepare features for prediction.\n",
    "    X_today = features_today[model_info['feature_columns']].fillna(0).values\n",
    "    X_today_scaled = model_info['scaler'].transform(X_today)\n",
    "    X_today_tensor = torch.tensor(X_today_scaled, dtype=torch.float32).to(model_info['device'])\n",
    "    \n",
    "    # Generate predictions.\n",
    "    model = model_info['model']\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_tensor = model(X_today_tensor)\n",
    "    predictions_array = predictions_tensor.cpu().numpy().flatten()\n",
    "    \n",
    "    # Add small noise to break ties.\n",
    "    predictions_array += np.random.normal(0, 1e-6, size=predictions_array.shape)\n",
    "    features_today['Predicted_Return'] = predictions_array\n",
    "    features_today['Signal_Rank'] = features_today['Predicted_Return'].rank(pct=True)\n",
    "    \n",
    "    predictions = features_today[['Ticker', 'Predicted_Return', 'Signal_Rank']].copy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91da3dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Preprocessing sentiment data...\n",
      "Creating features from sentiment data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_16020\\4194473657.py:126: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ws = df.groupby(['Ticker', 'Date']).apply(weighted_sentiment_func).reset_index(name='weighted_sentiment')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging sentiment features with stock returns...\n",
      "Scaling features...\n",
      "Training the model...\n",
      "Epoch   0 | Train Loss: 0.004583 | Val Loss: 0.003311\n",
      "Epoch  20 | Train Loss: 0.001929 | Val Loss: 0.002262\n",
      "Epoch  40 | Train Loss: 0.001769 | Val Loss: 0.002071\n",
      "Epoch  60 | Train Loss: 0.001740 | Val Loss: 0.002040\n",
      "Epoch  80 | Train Loss: 0.001728 | Val Loss: 0.002026\n",
      "Epoch 100 | Train Loss: 0.001721 | Val Loss: 0.002021\n",
      "Epoch 120 | Train Loss: 0.001717 | Val Loss: 0.002014\n",
      "Epoch 140 | Train Loss: 0.001713 | Val Loss: 0.002012\n",
      "Epoch 160 | Train Loss: 0.001711 | Val Loss: 0.002009\n",
      "Epoch 180 | Train Loss: 0.001708 | Val Loss: 0.002008\n",
      "Epoch 200 | Train Loss: 0.001706 | Val Loss: 0.002007\n",
      "Epoch 220 | Train Loss: 0.001704 | Val Loss: 0.002006\n",
      "Epoch 240 | Train Loss: 0.001703 | Val Loss: 0.002006\n",
      "Epoch 260 | Train Loss: 0.001701 | Val Loss: 0.002006\n",
      "Epoch 280 | Train Loss: 0.001700 | Val Loss: 0.002009\n",
      "Epoch 300 | Train Loss: 0.001699 | Val Loss: 0.002009\n",
      "Epoch 320 | Train Loss: 0.001698 | Val Loss: 0.002009\n",
      "Epoch 340 | Train Loss: 0.001697 | Val Loss: 0.002011\n",
      "Epoch 360 | Train Loss: 0.001697 | Val Loss: 0.002012\n",
      "Epoch 380 | Train Loss: 0.001696 | Val Loss: 0.002014\n",
      "Epoch 400 | Train Loss: 0.001695 | Val Loss: 0.002001\n",
      "Epoch 420 | Train Loss: 0.001695 | Val Loss: 0.002002\n",
      "Epoch 440 | Train Loss: 0.001694 | Val Loss: 0.002003\n",
      "Epoch 460 | Train Loss: 0.001694 | Val Loss: 0.002003\n",
      "Epoch 480 | Train Loss: 0.001693 | Val Loss: 0.002002\n",
      "Epoch 500 | Train Loss: 0.001693 | Val Loss: 0.002003\n",
      "Epoch 520 | Train Loss: 0.001693 | Val Loss: 0.002005\n",
      "Epoch 540 | Train Loss: 0.001692 | Val Loss: 0.002003\n",
      "Epoch 560 | Train Loss: 0.001692 | Val Loss: 0.001999\n",
      "Epoch 580 | Train Loss: 0.001692 | Val Loss: 0.002005\n",
      "Epoch 600 | Train Loss: 0.001692 | Val Loss: 0.001998\n",
      "Epoch 620 | Train Loss: 0.001691 | Val Loss: 0.002009\n",
      "Epoch 640 | Train Loss: 0.001691 | Val Loss: 0.002002\n",
      "Epoch 660 | Train Loss: 0.001691 | Val Loss: 0.002012\n",
      "Epoch 680 | Train Loss: 0.001690 | Val Loss: 0.002007\n",
      "Epoch 700 | Train Loss: 0.001690 | Val Loss: 0.001998\n",
      "Epoch 720 | Train Loss: 0.001691 | Val Loss: 0.001992\n",
      "Epoch 740 | Train Loss: 0.001690 | Val Loss: 0.002004\n",
      "Epoch 760 | Train Loss: 0.001689 | Val Loss: 0.002003\n",
      "Epoch 780 | Train Loss: 0.001690 | Val Loss: 0.002010\n",
      "Epoch 800 | Train Loss: 0.001689 | Val Loss: 0.002004\n",
      "Epoch 820 | Train Loss: 0.001689 | Val Loss: 0.001992\n",
      "Epoch 840 | Train Loss: 0.001689 | Val Loss: 0.001997\n",
      "Epoch 860 | Train Loss: 0.001688 | Val Loss: 0.001997\n",
      "Epoch 880 | Train Loss: 0.001690 | Val Loss: 0.002027\n",
      "Epoch 900 | Train Loss: 0.001688 | Val Loss: 0.002007\n",
      "Epoch 920 | Train Loss: 0.001688 | Val Loss: 0.001996\n",
      "Epoch 940 | Train Loss: 0.001688 | Val Loss: 0.002005\n",
      "Epoch 960 | Train Loss: 0.001689 | Val Loss: 0.002013\n",
      "Epoch 980 | Train Loss: 0.001689 | Val Loss: 0.001991\n",
      "Epoch 1000 | Train Loss: 0.001688 | Val Loss: 0.002009\n",
      "Epoch 1020 | Train Loss: 0.001687 | Val Loss: 0.001997\n",
      "Epoch 1040 | Train Loss: 0.001689 | Val Loss: 0.001990\n",
      "Epoch 1060 | Train Loss: 0.001687 | Val Loss: 0.002005\n",
      "Epoch 1080 | Train Loss: 0.001688 | Val Loss: 0.001990\n",
      "Epoch 1100 | Train Loss: 0.001688 | Val Loss: 0.002016\n",
      "Epoch 1120 | Train Loss: 0.001687 | Val Loss: 0.001991\n",
      "Epoch 1140 | Train Loss: 0.001687 | Val Loss: 0.001992\n",
      "Epoch 1160 | Train Loss: 0.001687 | Val Loss: 0.002018\n",
      "Epoch 1180 | Train Loss: 0.001686 | Val Loss: 0.001996\n",
      "Epoch 1200 | Train Loss: 0.001687 | Val Loss: 0.002012\n",
      "Epoch 1220 | Train Loss: 0.001687 | Val Loss: 0.002014\n",
      "Epoch 1240 | Train Loss: 0.001686 | Val Loss: 0.002000\n",
      "Epoch 1260 | Train Loss: 0.001689 | Val Loss: 0.001991\n",
      "Epoch 1280 | Train Loss: 0.001686 | Val Loss: 0.001999\n",
      "Epoch 1300 | Train Loss: 0.001686 | Val Loss: 0.001998\n",
      "Epoch 1320 | Train Loss: 0.001688 | Val Loss: 0.002039\n",
      "Epoch 1340 | Train Loss: 0.001686 | Val Loss: 0.001997\n",
      "Epoch 1360 | Train Loss: 0.001686 | Val Loss: 0.001992\n",
      "Epoch 1380 | Train Loss: 0.001685 | Val Loss: 0.001993\n",
      "Epoch 1400 | Train Loss: 0.001685 | Val Loss: 0.001996\n",
      "Epoch 1420 | Train Loss: 0.001686 | Val Loss: 0.001991\n",
      "Epoch 1440 | Train Loss: 0.001685 | Val Loss: 0.001999\n",
      "Epoch 1460 | Train Loss: 0.001688 | Val Loss: 0.002024\n",
      "Epoch 1480 | Train Loss: 0.001685 | Val Loss: 0.002008\n",
      "Epoch 1500 | Train Loss: 0.001685 | Val Loss: 0.001992\n",
      "Epoch 1520 | Train Loss: 0.001685 | Val Loss: 0.001990\n",
      "Epoch 1540 | Train Loss: 0.001686 | Val Loss: 0.002016\n",
      "Epoch 1560 | Train Loss: 0.001685 | Val Loss: 0.001992\n",
      "Epoch 1580 | Train Loss: 0.001685 | Val Loss: 0.002014\n",
      "Epoch 1600 | Train Loss: 0.001684 | Val Loss: 0.001992\n",
      "Epoch 1620 | Train Loss: 0.001685 | Val Loss: 0.001990\n",
      "Epoch 1640 | Train Loss: 0.001685 | Val Loss: 0.002009\n",
      "Epoch 1660 | Train Loss: 0.001685 | Val Loss: 0.001989\n",
      "Epoch 1680 | Train Loss: 0.001685 | Val Loss: 0.002011\n",
      "Epoch 1700 | Train Loss: 0.001684 | Val Loss: 0.001990\n",
      "Epoch 1720 | Train Loss: 0.001684 | Val Loss: 0.001990\n",
      "Epoch 1740 | Train Loss: 0.001686 | Val Loss: 0.001991\n",
      "Epoch 1760 | Train Loss: 0.001684 | Val Loss: 0.002004\n",
      "Epoch 1780 | Train Loss: 0.001684 | Val Loss: 0.001997\n",
      "Epoch 1800 | Train Loss: 0.001684 | Val Loss: 0.002005\n",
      "Epoch 1820 | Train Loss: 0.001685 | Val Loss: 0.002010\n",
      "Epoch 1840 | Train Loss: 0.001684 | Val Loss: 0.001990\n",
      "Epoch 1860 | Train Loss: 0.001685 | Val Loss: 0.002016\n",
      "Epoch 1880 | Train Loss: 0.001683 | Val Loss: 0.001995\n",
      "Epoch 1900 | Train Loss: 0.001683 | Val Loss: 0.001997\n",
      "Epoch 1920 | Train Loss: 0.001688 | Val Loss: 0.002040\n",
      "Epoch 1940 | Train Loss: 0.001683 | Val Loss: 0.001993\n",
      "Epoch 1960 | Train Loss: 0.001683 | Val Loss: 0.001994\n",
      "Epoch 1980 | Train Loss: 0.001687 | Val Loss: 0.001994\n",
      "Training complete. Model is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_16020\\4194473657.py:126: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ws = df.groupby(['Ticker', 'Date']).apply(weighted_sentiment_func).reset_index(name='weighted_sentiment')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 10)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m sentiment_data_today \u001b[38;5;241m=\u001b[39m preprocessed_sentiment[preprocessed_sentiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m sample_day]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     19\u001b[0m stock_universe_today \u001b[38;5;241m=\u001b[39m return_data[return_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m sample_day][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 21\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_returns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentiment_data_today\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstock_universe_today\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[23], line 318\u001b[0m, in \u001b[0;36mpredict_returns\u001b[1;34m(model_info, sentiment_data_today, stock_universe_today, historical_data)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# Prepare features for prediction.\u001b[39;00m\n\u001b[0;32m    317\u001b[0m X_today \u001b[38;5;241m=\u001b[39m features_today[model_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_columns\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 318\u001b[0m X_today_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscaler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_today\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m X_today_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_today_scaled, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(model_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Generate predictions.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1134\u001b[0m         )\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 10)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        sentiment_data = pd.read_csv('./data/sentiment_train_2017_2021.csv')\n",
    "        return_data = pd.read_csv('./data/return_train_2017_2021.csv')\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Data files not found. Please check file paths.\")\n",
    "        sentiment_data = None\n",
    "        return_data = None\n",
    "\n",
    "    if sentiment_data is not None and return_data is not None:\n",
    "        # Train the model.\n",
    "        model_info = train_model(sentiment_data, return_data)\n",
    "        \n",
    "        # For testing prediction, ensure the sentiment data is filtered to the sample day.\n",
    "        sample_day = pd.to_datetime('2021-06-01')\n",
    "        preprocessed_sentiment = preprocess_sentiment_data(sentiment_data)\n",
    "        sentiment_data_today = preprocessed_sentiment[preprocessed_sentiment['Date'] == sample_day].copy()\n",
    "        stock_universe_today = return_data[return_data['Date'] == sample_day]['Ticker'].unique().tolist()\n",
    "        \n",
    "        predictions = predict_returns(model_info, sentiment_data_today, stock_universe_today)\n",
    "        print(\"Sample predictions:\")\n",
    "        print(predictions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc94ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe0ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6254db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Preprocessing sentiment data...\n",
      "Creating features from sentiment data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_16020\\244528443.py:134: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ws = df.groupby(['Ticker', 'Date']).apply(weighted_sentiment_func).reset_index(name='weighted_sentiment')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging sentiment features with stock returns...\n",
      "Scaling features...\n",
      "Training the model...\n",
      "Epoch   0 | Train Loss: 0.032584 | Val Loss: 0.028987\n",
      "Epoch  20 | Train Loss: 0.003480 | Val Loss: 0.006543\n",
      "Epoch  40 | Train Loss: 0.001992 | Val Loss: 0.002861\n",
      "Epoch  60 | Train Loss: 0.001861 | Val Loss: 0.002181\n",
      "Epoch  80 | Train Loss: 0.001805 | Val Loss: 0.002135\n",
      "Epoch 100 | Train Loss: 0.001778 | Val Loss: 0.002100\n",
      "Epoch 120 | Train Loss: 0.001764 | Val Loss: 0.002083\n",
      "Epoch 140 | Train Loss: 0.001754 | Val Loss: 0.002072\n",
      "Epoch 160 | Train Loss: 0.001747 | Val Loss: 0.002063\n",
      "Epoch 180 | Train Loss: 0.001741 | Val Loss: 0.002056\n",
      "Training complete. Model is ready.\n",
      "Sample predictions:\n",
      "  Ticker  Predicted_Return  Signal_Rank\n",
      "0      A          0.005638     0.954362\n",
      "1     AA         -0.026684     0.067785\n",
      "2   AABB         -0.026683     0.517953\n",
      "3    AAC         -0.026682     0.609396\n",
      "4   AAIC          0.002029     0.897315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukea\\AppData\\Local\\Temp\\ipykernel_16020\\244528443.py:134: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ws = df.groupby(['Ticker', 'Date']).apply(weighted_sentiment_func).reset_index(name='weighted_sentiment')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "001_prediction_function.py\n",
    "\n",
    "This file provides two main functions:\n",
    "    1. train_model(sentiment_data, return_data)\n",
    "    2. predict_returns(model, sentiment_data_today, stock_universe_today)\n",
    "\n",
    "The code processes Alexandria’s Reddit sentiment data, creates aggregated features, and trains a PyTorch feedforward neural network \n",
    "(with GPU support when available) to predict next-day stock returns.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime, time, timedelta\n",
    "\n",
    "# Set device for GPU usage if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "####################################\n",
    "# Helper Functions\n",
    "####################################\n",
    "\n",
    "def convert_return(x):\n",
    "    \"\"\"\n",
    "    Converts a return value to a float.\n",
    "    If x is a string ending in '%', removes the '%' and divides by 100.\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if x.endswith('%'):\n",
    "            return float(x[:-1].strip()) / 100.0\n",
    "        else:\n",
    "            return float(x)\n",
    "    else:\n",
    "        return float(x)\n",
    "\n",
    "def preprocess_sentiment_data(sentiment_data):\n",
    "    \"\"\"\n",
    "    Preprocess sentiment data:\n",
    "      - Convert 'Received_Time' to a timezone-aware datetime (UTC) then to US/Eastern.\n",
    "      - Create a 'Date' column by shifting posts received after 4:00 PM (EST) to the next day.\n",
    "      - Ensure 'Ticker' is uppercase.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentiment_data : DataFrame\n",
    "        Raw sentiment data containing at least 'Received_Time' and 'Ticker'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        Processed sentiment data with added 'Received_Time_EST' and 'Date' columns.\n",
    "    \"\"\"\n",
    "    df = sentiment_data.copy()\n",
    "    if 'Received_Time' not in df.columns:\n",
    "        raise ValueError(\"Column 'Received_Time' not found in the sentiment data.\")\n",
    "    df['Received_Time'] = pd.to_datetime(df['Received_Time'], utc=True)\n",
    "    df['Received_Time_EST'] = df['Received_Time'].dt.tz_convert('America/New_York')\n",
    "    df['local_date'] = df['Received_Time_EST'].dt.date\n",
    "    cutoff = time(16, 0)  # 4:00 PM cutoff.\n",
    "    df['Date'] = df['Received_Time_EST'].apply(\n",
    "        lambda x: pd.to_datetime(x.date() + timedelta(days=1)) if x.time() > cutoff else pd.to_datetime(x.date())\n",
    "    )\n",
    "    if 'Ticker' in df.columns:\n",
    "        df['Ticker'] = df['Ticker'].str.upper()\n",
    "    return df\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create advanced daily features by aggregating sentiment data for each Ticker and Date.\n",
    "    \n",
    "    Features include:\n",
    "      - sentiment_mean: Average sentiment.\n",
    "      - sentiment_std: Standard deviation of sentiment.\n",
    "      - post_count: Count of posts.\n",
    "      - avg_confidence: Average confidence.\n",
    "      - avg_prob_pos, avg_prob_ntr, avg_prob_neg: Averages of probabilities.\n",
    "      - avg_source_weight, avg_topic_weight: Averages from respective columns.\n",
    "      - weighted_sentiment: Average (Sentiment * Relevance) per post.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Preprocessed sentiment data with columns including \n",
    "        ['Ticker', 'Date', 'Sentiment', 'Confidence', 'Prob_POS', 'Prob_NTR', 'Prob_NEG',\n",
    "         'Relevance', 'SourceWeight', 'TopicWeight'].\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grouped : DataFrame\n",
    "        Aggregated features for each Ticker and Date.\n",
    "    \"\"\"\n",
    "    required_columns = ['Ticker', 'Date', 'Sentiment', 'Confidence', 'Prob_POS', \n",
    "                        'Prob_NTR', 'Prob_NEG', 'Relevance', 'SourceWeight', 'TopicWeight']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            if col in ['Ticker', 'Date', 'Sentiment']:\n",
    "                raise ValueError(f\"Required column {col} is missing in the data.\")\n",
    "            else:\n",
    "                df[col] = 0\n",
    "    agg_funcs = {\n",
    "        'Sentiment': ['mean', 'std', 'count'],\n",
    "        'Confidence': 'mean',\n",
    "        'Prob_POS': 'mean',\n",
    "        'Prob_NTR': 'mean',\n",
    "        'Prob_NEG': 'mean',\n",
    "        'Relevance': 'mean',\n",
    "        'SourceWeight': 'mean',\n",
    "        'TopicWeight': 'mean'\n",
    "    }\n",
    "    grouped = df.groupby(['Ticker', 'Date']).agg(agg_funcs)\n",
    "    grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "    grouped = grouped.reset_index()\n",
    "    grouped = grouped.rename(columns={\n",
    "        'Sentiment_mean': 'sentiment_mean',\n",
    "        'Sentiment_std': 'sentiment_std',\n",
    "        'Sentiment_count': 'post_count',\n",
    "        'Confidence_mean': 'avg_confidence',\n",
    "        'Prob_POS_mean': 'avg_prob_pos',\n",
    "        'Prob_NTR_mean': 'avg_prob_ntr',\n",
    "        'Prob_NEG_mean': 'avg_prob_neg',\n",
    "        'SourceWeight_mean': 'avg_source_weight',\n",
    "        'TopicWeight_mean': 'avg_topic_weight',\n",
    "        'Relevance_mean': 'avg_relevance'\n",
    "    })\n",
    "    def weighted_sentiment_func(sub_df):\n",
    "        if sub_df['Sentiment'].count() == 0:\n",
    "            return 0\n",
    "        return (sub_df['Sentiment'] * sub_df['Relevance']).sum() / sub_df['Sentiment'].count()\n",
    "    ws = df.groupby(['Ticker', 'Date']).apply(weighted_sentiment_func).reset_index(name='weighted_sentiment')\n",
    "    grouped = pd.merge(grouped, ws, on=['Ticker', 'Date'], how='left')\n",
    "    return grouped\n",
    "\n",
    "####################################\n",
    "# PyTorch Neural Network Model\n",
    "####################################\n",
    "\n",
    "class FeedforwardNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        A simple feedforward neural network with two hidden layers.\n",
    "        \"\"\"\n",
    "        super(FeedforwardNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "####################################\n",
    "# Main Functions\n",
    "####################################\n",
    "\n",
    "def train_model(sentiment_data, return_data):\n",
    "    \"\"\"\n",
    "    Train a model using sentiment features to predict next-day returns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentiment_data : DataFrame\n",
    "        The Reddit sentiment data for training.\n",
    "    return_data : DataFrame\n",
    "        The stock return data for training.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model_info : dict\n",
    "        Contains the trained PyTorch model, scaler, feature columns, and device information.\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing sentiment data...\")\n",
    "    sentiment_data = preprocess_sentiment_data(sentiment_data)\n",
    "    print(\"Creating features from sentiment data...\")\n",
    "    features_df = create_features(sentiment_data)\n",
    "    \n",
    "    # Preprocess return_data: convert Date to datetime and normalize, Ticker to uppercase, and convert Return.\n",
    "    return_data = return_data.copy()\n",
    "    return_data['Date'] = pd.to_datetime(return_data['Date']).dt.normalize()\n",
    "    return_data['Ticker'] = return_data['Ticker'].str.upper()\n",
    "    return_data['Return'] = return_data['Return'].apply(convert_return)\n",
    "    \n",
    "    print(\"Merging sentiment features with stock returns...\")\n",
    "    model_data = pd.merge(features_df, return_data[['Date', 'Ticker', 'Return']], on=['Date', 'Ticker'], how='inner')\n",
    "    model_data = model_data.dropna(subset=['Return'])\n",
    "    \n",
    "    model_data['Return'] = model_data['Return'].apply(convert_return).astype(float)\n",
    "    feature_columns = ['sentiment_mean', 'sentiment_std', 'post_count', 'avg_confidence', \n",
    "                         'avg_prob_pos', 'avg_prob_ntr', 'avg_prob_neg', 'weighted_sentiment', \n",
    "                         'avg_source_weight', 'avg_topic_weight']\n",
    "    model_data[feature_columns] = model_data[feature_columns].fillna(0)\n",
    "    \n",
    "    model_data = model_data.sort_values('Date')\n",
    "    unique_dates = np.sort(model_data['Date'].unique())\n",
    "    split_index = int(0.8 * len(unique_dates))\n",
    "    train_dates = unique_dates[:split_index]\n",
    "    val_dates = unique_dates[split_index:]\n",
    "    \n",
    "    train_data = model_data[model_data['Date'].isin(train_dates)]\n",
    "    val_data = model_data[model_data['Date'].isin(val_dates)]\n",
    "    \n",
    "    X_train = train_data[feature_columns].values\n",
    "    y_train = train_data['Return'].values.reshape(-1, 1)\n",
    "    X_val = val_data[feature_columns].values\n",
    "    y_val = val_data['Return'].values.reshape(-1, 1)\n",
    "    \n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "    \n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "    model_net = FeedforwardNet(input_dim).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model_net.parameters(), lr=0.001)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    epochs = 200\n",
    "    for epoch in range(epochs):\n",
    "        model_net.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_net(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            model_net.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model_net(X_val_tensor)\n",
    "                val_loss = criterion(val_outputs, y_val_tensor)\n",
    "            print(f\"Epoch {epoch:3d} | Train Loss: {loss.item():.6f} | Val Loss: {val_loss.item():.6f}\")\n",
    "    \n",
    "    model_info = {\n",
    "        'model': model_net,\n",
    "        'scaler': scaler,\n",
    "        'feature_columns': feature_columns,\n",
    "        'device': device\n",
    "    }\n",
    "    print(\"Training complete. Model is ready.\")\n",
    "    return model_info\n",
    "\n",
    "def predict_returns(model, sentiment_data_today, stock_universe_today):\n",
    "    \"\"\"\n",
    "    Generate predictions of next-day returns for all stocks in the universe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : dict\n",
    "        Contains the trained model, scaler, feature columns, and device info.\n",
    "    sentiment_data_today : DataFrame\n",
    "        New sentiment data (for a single day).\n",
    "    stock_universe_today : list\n",
    "        List of stock tickers available today.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predictions : DataFrame\n",
    "        A DataFrame with columns ['Ticker', 'Predicted_Return', 'Signal_Rank'].\n",
    "    \"\"\"\n",
    "    # Preprocess today's sentiment data.\n",
    "    sentiment_data_today = preprocess_sentiment_data(sentiment_data_today)\n",
    "    \n",
    "    # Define current_date: if no sentiment data is present, use today's normalized date; otherwise, the max Date.\n",
    "    if sentiment_data_today.empty:\n",
    "        current_date = pd.Timestamp.today().normalize()\n",
    "    else:\n",
    "        current_date = sentiment_data_today['Date'].max()\n",
    "    \n",
    "    # Filter to today's sentiment data and create features.\n",
    "    sentiment_today = sentiment_data_today[sentiment_data_today['Date'] == current_date].copy()\n",
    "    features_today = create_features(sentiment_today)\n",
    "    \n",
    "    # Convert provided stock universe tickers to uppercase.\n",
    "    universe_upper = [t.upper() for t in stock_universe_today]\n",
    "    \n",
    "    # If features_today is empty or missing the 'Ticker' column, create default rows.\n",
    "    if features_today.empty or 'Ticker' not in features_today.columns:\n",
    "        default_data = []\n",
    "        for t in universe_upper:\n",
    "            default_row = {col: 0 for col in model['feature_columns']}\n",
    "            default_row['Ticker'] = t\n",
    "            default_row['Date'] = current_date\n",
    "            default_data.append(default_row)\n",
    "        features_today = pd.DataFrame(default_data)\n",
    "    else:\n",
    "        features_today['Ticker'] = features_today['Ticker'].str.upper()\n",
    "        features_today = features_today[features_today['Ticker'].isin(universe_upper)]\n",
    "        # Add missing tickers if needed.\n",
    "        existing_tickers = set(features_today['Ticker'])\n",
    "        missing_tickers = set(universe_upper) - existing_tickers\n",
    "        if missing_tickers:\n",
    "            default_data = []\n",
    "            for t in missing_tickers:\n",
    "                default_row = {col: 0 for col in model['feature_columns']}\n",
    "                default_row['Ticker'] = t\n",
    "                default_row['Date'] = current_date\n",
    "                default_data.append(default_row)\n",
    "            if default_data:\n",
    "                default_df = pd.DataFrame(default_data)\n",
    "                features_today = pd.concat([features_today, default_df], ignore_index=True)\n",
    "    \n",
    "    # Ensure required feature columns exist.\n",
    "    for col in model['feature_columns']:\n",
    "        if col not in features_today.columns:\n",
    "            features_today[col] = 0\n",
    "    \n",
    "    if 'Ticker' not in features_today.columns:\n",
    "        features_today['Ticker'] = \"\"\n",
    "    \n",
    "    features_today = features_today.sort_values('Ticker').reset_index(drop=True)\n",
    "    \n",
    "    X_today = features_today[model['feature_columns']].fillna(0).values\n",
    "    if X_today.shape[0] == 0:\n",
    "        X_today = np.zeros((len(universe_upper), len(model['feature_columns'])))\n",
    "        features_today = pd.DataFrame({'Ticker': universe_upper})\n",
    "        for col in model['feature_columns']:\n",
    "            features_today[col] = 0\n",
    "\n",
    "    X_today_scaled = model['scaler'].transform(X_today)\n",
    "    X_today_tensor = torch.tensor(X_today_scaled, dtype=torch.float32).to(model['device'])\n",
    "    \n",
    "    model_obj = model['model']\n",
    "    model_obj.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_tensor = model_obj(X_today_tensor)\n",
    "    predictions_array = predictions_tensor.cpu().numpy().flatten()\n",
    "    \n",
    "    predictions_array += np.random.normal(0, 1e-6, size=predictions_array.shape)\n",
    "    features_today['Predicted_Return'] = predictions_array\n",
    "    features_today['Signal_Rank'] = features_today['Predicted_Return'].rank(pct=True)\n",
    "    \n",
    "    predictions = features_today[['Ticker', 'Predicted_Return', 'Signal_Rank']].copy()\n",
    "    return predictions\n",
    "\n",
    "####################################\n",
    "# Test Section (Runs if the script is executed directly)\n",
    "####################################\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        sentiment_data = pd.read_csv('./data/sentiment_train_2017_2021.csv')\n",
    "        return_data = pd.read_csv('./data/return_train_2017_2021.csv')\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Data files not found. Please check file paths.\")\n",
    "        sentiment_data = None\n",
    "        return_data = None\n",
    "\n",
    "    if sentiment_data is not None and return_data is not None:\n",
    "        # Normalize the return_data Date column.\n",
    "        return_data['Date'] = pd.to_datetime(return_data['Date']).dt.normalize()\n",
    "        \n",
    "        model_info = train_model(sentiment_data, return_data)\n",
    "        \n",
    "        sample_day = pd.to_datetime('2021-06-01').normalize()\n",
    "        preprocessed_sentiment = preprocess_sentiment_data(sentiment_data)\n",
    "        sentiment_data_today = preprocessed_sentiment[preprocessed_sentiment['Date'] == sample_day].copy()\n",
    "        \n",
    "        stock_universe_today = return_data[return_data['Date'] == sample_day]['Ticker'].unique().tolist()\n",
    "        \n",
    "        # Call predict_returns using keyword arguments matching the required signature.\n",
    "        predictions = predict_returns(model=model_info, sentiment_data_today=sentiment_data_today, stock_universe_today=stock_universe_today)\n",
    "        print(\"Sample predictions:\")\n",
    "        print(predictions.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
